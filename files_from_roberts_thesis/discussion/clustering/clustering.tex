\section{Clustering of AT-TPC events}

From the semi-supervised results, we know that we can construct class separating latent spaces. The next challenge is then to expand that insight into the unsupervised clustering of events. Clustering is an intrinsically harder task, owing to the unsupervised nature of the problem. This leads to differing requirements on the representation of the data, as a linearly separable latent space is not necessarily suitable for clustering. 

The most naive approach to clustering our data is to apply a simple clustering algorithm on the latent representation of an algorithm which we know separates our data. Using a pre-trained model is advantageous because they are generally well studied, but may offer less flexibility.  A more sophisticated approach is to use algorithms designed for clustering. These will invariably carry their own assumptions and pitfalls that we have to be wary of. 

In this thesis, we have applied a K-means algorithm to the latent space of a pre-trained VGG16 model as our naive approach. Which means that the K-means assumptions bound this approach. These include that the clusters are distributed as Gaussians in the high-dimensional latent space and that we have a good idea of the number of clusters. We also investigate the more sophisticated mixture of autoencoder (MIXAE) and deep convolutional embedded clustering (DCEC) models, the latter of which fails to cluster AT-TPC data to any degree. However, the MIXAE model achieves promising results, but assumes uniformly distributed classes and requires a good idea of the number of clusters present in the data.

The assumption that we have a good idea of the number of clusters present is problematic for AT-TPC data. In the filtered and full datasets, the amorphous "other" class probably consist of a variety of event types. These were empirically placed in the same class for classification purposes when labelling the data, precisely because the researchers were unable to identify these events.

In this section, we will explore and discuss the clustering of AT-TPC data using a pre-trained network and the MIXAE model. We begin by considering the results from clustering on the pre-trained VGG16 model latent space. 

\subsection{Clustering with a pre-trained network}

What is immediately clear from the results in table \ref{tab:clstr_vgg} is that the VGG16 latent space is able to capture difference between event classes, achieving an adjusted rand index (ARI) score of $>0.88$ on the simulated data against the ground-truth labels. Recall that the ARI is a measure of clustering similarities, adjusted for chance. A particularly interesting aspect of the results in table \ref{tab:clstr_vgg} is the ability to cluster despite the very high-dimensional latent space. As previously mentioned, Euclidean vector distances can be uninformative in high dimensional spaces, as shown by \cite{Aggarwal}. We can explain a part of this behaviour by the fact that a majority, on the order of $\sim 80\%$, of the entries in the latent vectors are zero-valued. However, that leaves some $\sim \num{1.5e3}$ non-zero elements per sample. This space is still very much a high dimensional representation. For the Euclidean distances to have discriminatory power, we must then infer that the space contains some very dense regions in-between regions of minimal density. In such a configuration, the ratios between distances will not tend to unity, and we find some success with K-means clustering. The results for the filtered and full data show worse performance than for the simulated data. However, these results are still promising for the application of clustering algorithms to pre-trained network latent spaces.

To further characterize the performance, we then consider the confusion matrices shown in figure \ref{fig:clster_confmat}. From these matrices, we can infer some interesting properties about the types of events placed in different clusters. It seems that the clustering of the filtered data identifies proton events that occur closer to the detector plane, and so have less developed spirals. We confirm this by visually inspecting the events in those clusters, as illustrated in figure \ref{fig:filtered_vgg_clster_repr}. Similarly, we infer that clustering of full events segments the proton class by the noisiness of the event, placing the more noisy proton events with the amorphous "other" category. We confirm this argument by inspecting the events contained in each cluster, illustrated in figure \ref{fig:full_vgg_clster_repr}.

One of the major strengths of this application is the consistency of the clustering algorithms. The K-means algorithm showed very little, if any, variation in performance when re-running the algorithm. 

\subsection{Clustering with autoencoder based models}

In addition to the results from the VGG16+K-means algorithm, we implemented and applied the DCEC and MIXAE algorithms to the ${}^{46}$Ar data. The former failed to produce any salient clusterings of AT-TPC data. We are confident in the implementation of the algorithm as we are able to reproduce the original author's results on the MNIST handwritten digits dataset. Some value was gained from the experiments, as the mode of failure was informative for the choice to implement the MIXAE algorithm. The DCEC would consistently collapse all samples to one cluster. This exact mode of failure is addressed by the batch entropy loss term introduced in the MIXAE algorithm. Recall from equation \ref{eq:mixae_batch} that the batch entropy encourages the clustering probabilities to be diverse. This also explains the relative imbalance between the weighing of the losses that we found in section \ref{sec:convae_clustering}.

We then turn to the MIXAE performance shown in figures \ref{fig:mixae_sim}, \ref{fig:mixae_clean} and \ref{fig:mixae_full}. From these figures, we observe that while the MIXAE algorithm is able to achieve better performance than the VGG16+K-means algorithm, it suffers from severe stability problems. The results for the simulated data are noteworthy in this regard. To better understand the out of sample performance for the clustering algorithm, we segregate the training and test data entirely for the simulated data. Coupled with the high variability in the simulated performance, we infer that this method may not generalize well to unseen data. Being unsupervised, this is not as big a caveat for the MIXAE as it would be for a supervised algorithm.

Additionally, the best performing algorithms are systematically not those with the lowest loss-values. Indeed it seems that convergence to a perfect one-hot prediction of cluster belonging is indicative of the training having failed. This is especially clear for the filtered and full datasets whose performance are shown in figures \ref{fig:mixae_clean} and \ref{fig:mixae_full} respectively. A defining feature of the loss-plots is in the sample loss, as defined in equation \ref{eq:mixae_sample}. We observe that for the filtered and full datasets, the sample entropy is quite large for the models with the highest performance, with values $S_{sample} > 10$. Together with the observation that the batch-entropy is at a minimum, we conclude that the cluster assignments are near uniform. For comparison, a perfect one-hot representation has a sample loss of $\sim 10^{-1}$. The initially most plausible candidate for this behaviour is the class imbalance in the data, recall from table \ref{tab:class_distr} that the amorphous "other" class represents $\sim 60 \%$ of the full and filtered data. Indeed, we observe that the high performing models on the simulated data have an order of magnitude smaller sample loss compared to the models applied to full and filtered data. 

To further characterize the performance, we inspect the properties of the clustering results achieved by the highest performing models on the filtered and full datasets. The confusion matrices listed in figure \ref{fig:mixae_confmat} initially displays some of the same properties we observed for the VGG16+K-means algorithm. To confirm this behavior, we display some select events from the proton class in figures \ref{fig:filtered_mixae_clster_repr} and \ref{fig:full_mixae_clster_repr}. From these plots, we confirm that while the clustering of the full dataset is in agreement with the VGG16+K-means clusters. That is, we observe noisy proton events being placed in the cluster with the "other" class. This is not the case for the filtered dataset. In the filtered data, the different clusters of proton events do not seem to be visually distinguishable.

\subsection{Comparing clustering methods}

Both the pre-trained and autoencoder based clustering methods hold promise. As is the case for classification, the ease of application of the pre-trained methods is a significant boon, but their static nature creates a hard to breach cap on their performance. We also observe an impressive purity in the proton clusters, as shown in figure \ref{fig:clster_confmat}, for both the filtered and full datasets. Notably, this purity is absent from the MIXAE clustering results. The increase in performance from the VGG16+K-means is then a result of a stronger segmentation of the "other" class of events.


We also observe a link between the clustering and classifying autoencoders in their performance on real AT-TPC data. Recall that for the real data, both the MIXAE and VGG16+K-means approaches showed confusion over very noisy proton events. We observe the same behaviour in the classifying latent spaces of the autoencoder and VGG16 models illustrated by their t-SNE projections shown in figures \ref{fig:vgg_tsne}, \ref{fig:ac_tnse} and \ref{fig:vgg_ac_tnse}.  In those figures, the majority of proton events are correctly segmented from the rest of the data. A portion of the proton events does, however, intermingle with the "other" class in a way that is familiar to us from the clustering results.

It remains to be seen if the algorithms explored in this thesis are capable of separating events in future experiments with more similar tracks. A related avenue for future research is then to combine the autoencoder methods discussed here with a duelling decoder objective on Hough transformed events. This representation is promising as it can explicitly encode the geometry in the event.

For on-going experiments with the AT-TPC, our recommendation for clustering is to employ a pre-trained model combined with K-means. Performance validation is, unfortunately, a necessary step. Some positive identifications of events are thus needed to validate the clustering. 

In this thesis, we have demonstrated strong performance with the MIXAE algorithm. However, further inquiry is needed to investigate the stability of the algorithm. Other avenues of potential interest are the coupling of clustering with a duelling decoder objective, as well as other autoencoder based clustering algorithms. Lastly, there is a need for the coupling of unsupervised performance metrics to measures of performance against ground truth labels.  