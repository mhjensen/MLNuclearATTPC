\chapter{Experimental setup and design}

In this chapter, we describe the semi-supervised classification and clustering results obtained on the three active target time projection chamber (AT-TPC) datasets described in section \ref{sec:data}. The simulated data is used to provide a benchmark for the upper bound on the performance of a given algorithm. This chapter is structured by task first, and algorithm second. As such, we will begin with a consideration of the semi-supervised algorithms before continuing with the clustering task. 

 We explore the models proposed in chapter \ref{ch:autoencoder} on two disparate tasks, one of semi-supervised classification and one of clustering. For each task, we evaluate the performance on each of the datasets using appropriate metrics, which were introduced in section \ref{sec:performance_val}. 

 The primary objective for the ${}^{46}$Ar experiment was to identify resonant proton scattering events, but this thesis explores broader applications than this classification task. This broader picture is explicitly geared to the application of the models discussed in this thesis to other AT-TPC experiments. Following this argument, we measure individual class performance wherever appropriate. 

The machine learning experiments conducted in this thesis was performed using the AI-Hub computational cluster at the University of Oslo.  This resource consists of three machines with four RTX $2080$ Nvidia GPU's (graphics processing unit) each. These cards have $\sim 10$GB of memory available for the allocation of models.

\subsection{Semi-supervised classification procedure}\label{sec:clf_procedure}

Our interest in the semi-supervised classification is three fold. Firstly, it provides a bound on the performance we can expect from the clustering algorithms, as the classifier problem is fundamentally an easier task. In addition to the bounding property, the semi-supervised task proved to be an excellent way to explore the types of algorithms we present in this thesis. Secondly, we wish to characterize the performance as a function of the number of available labelled samples. As previously mentioned, one of the principal challenges with the AT-TPC detector is the cost of finding examples of the positive class, if at all possible. Thirdly, we wish to lay the groundwork for a transfer-learning approach to bridging AT-TPC experiments. Take, for example, resonant proton scattering. This could also be a reaction of interest in a different experiment. Applying models trained to recognize these events in the ${}^{46}$Ar experiment could be a feasible approach. The results from the semi-supervised classification experiments are presented in chapter \ref{chap:classification}

The semi-supervised task is to train an autoencoder model on a large dataset, and evaluate the class separation of the latent space using a logistic regression classifier. We use a logistic regression classifier as we wish to measure qualities of the latent space, and not of the classification per se. To provide an additional benchmark for our algorithms, we measure the performance of a logistic regression classifier using the latent space of a pre-trained image classifier model, which has shown to be effective in the classification of events from the ${}^{46}$Ar experiment \cite{Kuchera2019}.

Intrinsic to the measurement of the semi-supervised performance is the budgeting of how many labelled samples one can feasibly extract. Moreover, the principal limitation of the semi-supervised approach is the assumption that the researchers can identify the event class(es) of interest positively. It is then interesting to quantify the change in model performance as a function of how many labelled samples the classification model has to train on. Bear in mind that the representation that the classification model sees is still trained on the full set of events for a given dataset. 

For reference, the models are described in terms of their hyperparameters in table \ref{tab:convae_hyperparams} for the convolutional autoencoder and table \ref{tab:draw_hyperparams} for the DRAW-analogues. To determine the best hyperparameters, we perform on the order of $\sim 10^2$ runs, each taking on the order of $\sim 10^1$ minutes on a single Nvidia $2080$ GPU. For each configuration of the algorithms, we train a classifier on a subset of the labelled set. Subsequently, this classifier is evaluated on the remainder of the labelled data to estimate the out of sample (OOS) error. The best configuration is then lastly re-trained, and we evaluate the OOS error with k-fold cross validation as outlined in section \ref{sec:performance_val}.

\subsection{Clustering procedure}\label{sec:cster_procedure}

While the semi-supervised classification task provides context and understanding to the AT-TPC experiments, the clustering procedure is a direct attempt at solving challenges associated with these experiments. First and foremost is the challenge of acquiring labelled data of the event of interest. The clustering results are presented in chapter \ref{chap:clustering} 

In contrast to the semi-supervised task, the clustering objective presumes that we have access to no labelled data. However, since the purpose of this thesis is in large part exploratory, we will measure performance on the labelled data provided. We explore the performance of the deep clustering algorithms described in section \ref{sec:deep_clustering} as well as a K-means approach using the latent space of a pre-trained image classifier model. 

The clustering task is intrinsically a more challenging task than that of semi-supervised classification, and so more time was spent exploring algorithms to determine which were suitable for clustering AT-TPC data. Before training each of the algorithms in section \ref{sec:deep_clustering} was first configured to reproduce the results from their respective papers. Subsequently, they were applied to the AT-TPC data, with their autoencoder hyperparameters selected empirically from the semi-supervised task. This was done to enable a focus on the clustering components of the algorithm. In particular, we focus on the weighting parameters of the mixture of autoencoders (MIXAE) clustering algorithm.

