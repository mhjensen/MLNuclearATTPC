Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@book{burkard2012, 
    author = {R.E. Burkard and M. Dell'Amico and S. Martello},
    title = {Assignment Problems},
    publisher = {SIAM, Philadelphia, UA},
    year = {2012}
}    
@article{Aurisano2016,
	doi = {10.1088/1748-0221/11/09/p09001},
	url = {https://doi.org/10.1088%2F1748-0221%2F11%2F09%2Fp09001},
	year = 2016,
	month = {sep},
	publisher = {{IOP} Publishing},
	volume = {11},
	number = {09},
	pages = {P09001--P09001},
	author = {A. Aurisano and A. Radovic and D. Rocco and A. Himmel and M.D. Messier and E. Niner and G. Pawloski and F. Psihas and A. Sousa and P. Vahle},
	title = {A convolutional neural network neutrino event classifier},
	journal = {Journal of Instrumentation},
	abstract = {Convolutional neural networks (CNNs) have been widely   applied in the computer vision community to solve complex problems   in image recognition and analysis. We describe an application of the   CNN technology to the problem of identifying particle interactions in sampling   calorimeters used commonly in high energy physics and high energy   neutrino physics in particular. Following a discussion of the core   concepts of CNNs and recent innovations in CNN architectures related   to the field of deep learning, we outline a specific application to   the NOvA neutrino detector. This algorithm, CVN (Convolutional    Visual  Network) identifies neutrino   interactions based on their topology without the need for detailed   reconstruction and outperforms algorithms currently in use by the   NOvA collaboration.}
}

@inproceedings{Zhang,
abstract = {Unsupervised clustering is one of the most fundamental challenges in machine learning. A popular hypothesis is that data are generated from a union of low-dimensional nonlinear manifolds; thus an approach to clustering is identifying and separating these manifolds. In this paper, we present a novel approach to solve this problem by using a mixture of autoencoders. Our model consists of two parts: 1) a collection of autoencoders where each autoencoder learns the underlying manifold of a group of similar objects , and 2) a mixture assignment neural network, which takes the concatenated latent vectors from the autoencoders as input and infers the distribution over clusters. By jointly optimizing the two parts, we simultaneously assign data to clusters and learn the underlying manifolds of each cluster.},
archivePrefix = {arXiv},
arxivId = {1712.07788v2},
author = {Zhang, Dejiao and Sunm, Yifan and Eriksson, Brian and Balzano, Laura},
booktitle = {International Conference on Neural Information Processing},
eprint = {1712.07788v2},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Deep Unsupervised Clustering Using Mixture of Autoencoders.pdf:pdf},
pages = {373},
title = {{Deep Unsupervised Clustering Using Mixture of Autoencoders}},
url = {https://arxiv.org/pdf/1712.07788.pdf},
year = {2017}
}
@article{Suzuki2012,
abstract = {The Prototype AT-TPC, a detector based on time projection chamber (TPC) technology was built at the National Superconducting Cyclotron Laboratory, Michigan State University. The chamber gas, called the active target, of the Prototype AT-TPC is utilized simultaneously as a reaction target and a tracking medium of charged particles for measuring low-energy nuclear reactions. In pursuit of luminosity, efficiency and resolution for reaction studies at a new generation of radioactive isotope facilities, the Prototype AT-TPC provides one of the largest active volumes in the world measuring 25cm in diameter and 50cm long along the beam axis, millimeter-precision tracking capability, and a high electric field of 1kV/cm/atm for fast electron drift, which limits the detector dead time. Commissioning of the detector using standard alpha sources and radioactive 6He beams has demonstrated the detector's long-term stability as well as its performance, which allowed good reconstruction of reaction kinematics.},
author = {Suzuki, D. and Ford, M. and Bazin, D. and Mittig, W. and Lynch, W.G. and Ahn, T. and Aune, S. and Galyaev, E. and Fritsch, A. and Gilbert, J. and Montes, F. and Shore, A. and Yurkon, J. and Kolata, J.J. and Browne, J. and Howard, A. and Roberts, A.L. and Tang, X.D.},
doi = {10.1016/J.NIMA.2012.06.050},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Suzuki et al. - 2012 - Prototype AT-TPC Toward a new generation active target time projection chamber for radioactive beam experiment(2).pdf:pdf},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
pages = {39},
publisher = {North-Holland},
title = {{Prototype AT-TPC: Toward a new generation active target time projection chamber for radioactive beam experiments}},
volume = {691},
year = {2012}
}
@unpublished{Antoran2019,
abstract = {Learning representations that disentangle the underlying factors of variability in data is an intuitive precursor to AI with human-like reasoning. Consequently, it has been the object of many efforts of the machine learning community. This work takes a step further in this direction by addressing the scenario where generative factors present a multimodal distribution due to the existence of class distinction in the data. We formulate a lower bound on the joint distribution of inputs and class labels and present N-VAE, a model which is capable of separating factors of variation which are exclusive to certain classes from factors that are shared among classes. This model implements the natural clustering prior through the use of a class-conditioned latent space and a shared latent space. We show its usefulness for detecting and disentangling class-dependent generative factors as well as for generating rich artificial samples.},
archivePrefix = {arXiv},
arxivId = {1901.09415v1},
author = {Antor{\'{a}}n, Javier and Vivolab, Antonio Miguel},
booktitle = {Arxiv},
eprint = {1901.09415v1},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Antor{\'{a}}n, Vivolab - Unknown - DISENTANGLING IN VARIATIONAL AUTOENCODERS WITH NATURAL CLUSTERING.pdf:pdf},
title = {{DISENTANGLING IN VARIATIONAL AUTOENCODERS WITH NATURAL CLUSTERING}},
url = {https://arxiv.org/pdf/1901.09415.pdf},
year = {2019}
}
@article{Linnainmaa1976,
abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
author = {Linnainmaa, Seppo},
doi = {10.1007/BF01931367},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Linnainmaa - 1976 - Taylor expansion of the accumulated rounding error.pdf:pdf},
journal = {BIT},
month = {jun},
pages = {146},
publisher = {Kluwer Academic Publishers},
title = {{Taylor expansion of the accumulated rounding error}},
url = {http://link.springer.com/10.1007/BF01931367},
volume = {16},
year = {1976}
}
@article{Pearlmutter1989,
author = {Pearlmutter, Barak A.},
doi = {10.1162/neco.1989.1.2.263},
file = {:Users/solli/Downloads/learningstatespacetrajectoriesRNN.pdf:pdf},
journal = {Neural Computation},
pages = {263},
title = {{Learning State Space Trajectories in Recurrent Neural Networks}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1989.1.2.263},
volume = {1},
year = {1989}
}
@techreport{Steinbach,
abstract = {Cluster analysis divides data into groups (clusters) for the purposes of summarization or improved understanding. For example, cluster analysis has been used to group related documents for browsing, to find genes and proteins that have similar functionality, or as a means of data compression. While clustering has a long history and a large number of clustering techniques have been developed in statistics, pattern recognition, data mining, and other fields, significant challenges still remain. In this chapter we provide a short introduction to cluster analysis, and then focus on the challenge of clustering high dimensional data. We present a brief overview of several recent techniques, including a more detailed description of recent work of our own which uses a concept-based clustering approach.},
author = {Steinbach, Michael and Ert{\"{o}}z, Levent and Kumar, Vipin},
file = {::},
title = {{The Challenges of Clustering High Dimensional Data}},
url = {https://www-users.cs.umn.edu/{~}ertoz/papers/clustering{\_}chapter.pdf}
}
@article{Russakovsky2015,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
file = {::},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
pages = {211},
publisher = {Springer US},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {http://link.springer.com/10.1007/s11263-015-0816-y},
volume = {115},
year = {2015}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  doi = {10.1109/CVPR.2009.5206848},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}



@techreport{Harris2019,
abstract = {The ability to look multiple times through a series of pose-adjusted glimpses is fundamental to human vision. This critical faculty allows us to understand highly complex visual scenes. Short term memory plays an integral role in aggregating the information obtained from these glimpses and informing our interpretation of the scene. Computational models have attempted to address glimpsing and visual attention but have failed to incorporate the notion of memory. We introduce a novel, biologically inspired visual working memory architecture that we term the Hebb-Rosenblatt memory. We subsequently introduce a fully differentiable Short Term Attentive Working Memory model (STAWM) which uses transformational attention to learn a memory over each image it sees. The state of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space of a layer. By projecting different queries through this layer we can obtain goal-oriented latent representations for tasks including classification and visual reconstruction. Our model obtains highly competitive classification performance on MNIST and CIFAR-10. As demonstrated through the CelebA dataset, to perform reconstruction the model learns to make a sequence of updates to a canvas which constitute a parts-based representation. Classification with the self supervised representation obtained from MNIST is shown to be in line with the state of the art models (none of which use a visual attention mechanism). Finally, we show that STAWM can be trained under the dual constraints of classification and reconstruction to provide an interpretable visual sketchpad which helps open the 'black-box' of deep learning.},
archivePrefix = {arXiv},
arxivId = {1901.03665},
author = {Harris, Ethan and Niranjan, Mahesan and Hare, Jonathon},
eprint = {1901.03665},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Harris, Niranjan, Hare - 2019 - A Biologically Inspired Visual Working Memory for Deep Networks.pdf:pdf},
title = {{A Biologically Inspired Visual Working Memory for Deep Networks}},
url = {http://arxiv.org/abs/1901.03665 https://openreview.net/forum?id=B1fbosCcYm},
year = {2019}
}
@article{Hoerl1970,
abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X'X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X'X to obtain biased estimates with smaller mean square error.},
author = {Hoerl, Arthur E. and Kennard, Robert W.},
file = {::},
journal = {Technometrics},
pages = {55},
title = {{Ridge Regression: Biased Estimation for Nonorthogonal Problems}},
url = {https://www.math.arizona.edu/{~}hzhang/math574m/Read/RidgeRegressionBiasedEstimationForNonorthogonalProblems.pdf},
volume = {12},
year = {1970}
}
@article{Mehta2019,
abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias–variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton–proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute.},
author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching Hao and Day, Alexandre G.R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
doi = {10.1016/j.physrep.2019.03.001},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Mehta et al. - 2019 - A high-bias, low-variance introduction to Machine Learning for physicists(3).pdf:pdf},
journal = {Physics Reports},
pages = {1},
title = {{A high-bias, low-variance introduction to Machine Learning for physicists}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0370157319300766},
volume = {810},
year = {2019}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
eprint = {1603.07285},
month = {mar},
title = {{A guide to convolution arithmetic for deep learning}},
url = {http://arxiv.org/abs/1603.07285},
year = {2016}
}
@article{Hubert1985,
abstract = {The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Milligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from cOrresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between-4-I.},
author = {Hubert, Lawrence and Arabic, Phipps},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Hubert, Arabic - 1985 - Comparing Partitions.pdf:pdf},
journal = {Journal of Classification},
keywords = {Consensus indices,Measures of agreement,Measures of association},
pages = {193},
title = {{Comparing Partitions}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBF01908075.pdf},
volume = {2},
year = {1985}
}
@article{Rosenblatt1958,
author = {Rosenblatt, F},
file = {::},
journal = {Psychological Review},
pages = {19},
title = {{The perceptron: a probabilistic model for information storage and organization in the brain}},
url = {https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf},
volume = {65},
year = {1958}
}
@misc{Marsland2009,
abstract = {In this Techview, McLafferty reviews new applications of mass spectroscopy that are allowing new insights into protein and nuclei acid structure and function, including sequencing of peptides and DNA.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Marsland, Stephen},
booktitle = {CRC Press},
doi = {10.1126/science.284.5418.1289},
eprint = {0-387-31073-8},
isbn = {0036-8075},
issn = {00368075},
pmid = {10383309},
title = {{Machine Learning: An Algorithmic Perspective}},
year = {2009}
}
@article{Cybenko1989,
author = {Cybenko, G.},
doi = {10.1007/BF02551274},
file = {::},
issn = {0932-4194},
journal = {Mathematics of Control, Signals, and Systems},
month = {dec},
pages = {303},
publisher = {Springer-Verlag},
title = {{Approximation by superpositions of a sigmoidal function}},
url = {http://link.springer.com/10.1007/BF02551274},
volume = {2},
year = {1989}
}
@techreport{VanDerMaaten2008,
abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {{Van Der Maaten}, Laurens and Hinton, Geoffrey},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579},
title = {{Visualizing Data using t-SNE}},
url = {http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf},
volume = {9},
year = {2008}
}
@inproceedings{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlineari-ties. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normal-ization a part of the model architecture and performing the normalization for each training mini-batch. Batch Nor-malization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regu-larizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167v3},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {International conference on machine learning},
eprint = {1502.03167v3},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training b y Reducing Internal Covariate Shift.pdf:pdf},
pages = {11},
title = {{Batch Normalization: Accelerating Deep Network Training b y Reducing Internal Covariate Shift}},
url = {https://pdfs.semanticscholar.org/b58f/1529c22d682dbe08ae02ec52587c9da7f270.pdf},
year = {2015}
}
@techreport{Fertig,
abstract = {In this paper, we investigate the degree to which the encoding of a {\$}\backslashbeta{\$}-VAE captures label information across multiple architectures on Binary Static MNIST and Omniglot. Even though they are trained in a completely unsupervised manner, we demonstrate that a {\$}\backslashbeta{\$}-VAE can retain a large amount of label information, even when asked to learn a highly compressed representation.},
archivePrefix = {arXiv},
arxivId = {1812.02682},
author = {Fertig, Emily and Arbabi, Aryan and Alemi, Alexander A.},
eprint = {1812.02682},
file = {::},
title = {{beta-VAEs can retain label information even at high compression}},
url = {http://arxiv.org/abs/1812.02682},
year = {2018}
}
@article{Lecun1998,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
journal = {Proceedings of the IEEE},
pages = {2278},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/document/726791/},
volume = {86},
year = {1998}
}
@incollection{Aggarwal,
abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1 norm) is consistently more preferable than the Euclidean distance metric L(2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
doi = {10.1007/3-540-44503-x_27},
file = {::},
pages = {420--434},
title = {{On the Surprising Behavior of Distance Metrics in High Dimensional Space}},
url = {https://bib.dbvis.de/uploadedFiles/155.pdf},
year = {2001}
}
@techreport{Krane1988,
author = {Krane, Kenneth S and Wlley, John and York, New and Brisbane, Chichester and Singapore, Toronto},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Krane et al. - 1988 - INTRODUCTORY NUCLEAR PHYSICS(7).pdf:pdf},
title = {{INTRODUCTORY NUCLEAR PHYSICS}},
url = {http://faculty.kfupm.edu.sa/PHYS/aanaqvi/Introductory-Nuclear-Physics-new-Krane.pdf},
year = {1988}
}
@misc{Karpathy,
abstract = {In the previous sections we've discussed the static parts of a Neural Networks: how we can set up the network connectivity, the data, and the loss function. This section is devoted to the dynamics, or in other words, the process of learning the parameters and finding good hyperparameters.},
author = {Karpathy, Andrej},
booktitle = {Open source},
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
url = {https://cs231n.github.io/neural-networks-3/},
urldate = {2019-06-05},
year = {2019}
}
@techreport{Frankle2019,
abstract = {Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the lottery ticket hypothesis conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1{\%} to 7{\%} through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80{\%} sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork stability, finding that-as accuracy improves in this fashion-IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis.},
archivePrefix = {arXiv},
arxivId = {1903.01611v2},
author = {Frankle, Jonathan and Dziugaite, Karolina and Roy, Daniel M and Carbin, Michael},
eprint = {1903.01611v2},
file = {::},
title = {{Stabilizing the Lottery Ticket Hypothesis}},
url = {https://arxiv.org/pdf/1903.01611.pdf},
year = {2019}
}
@techreport{Arvanitakis,
abstract = {In this paper, we develop an unsupervised generative clustering framework that combines variational information bottleneck and the Gaussian Mixture Model. Specifically, in our approach we use the variational information bottleneck method and model the latent space as a mixture of Gaussians. We derive a bound on the cost function of our model that generalizes the evidence lower bound (ELBO); and provide a variational inference type algorithm that allows to compute it. In the algorithm, the coders' mappings are parametrized using neural networks and the bound is approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on real datasets are provided to support the efficiency of our method.},
archivePrefix = {arXiv},
arxivId = {1905.11741v1},
author = {Arvanitakis, George and Zaidi, Abdellatif and Ugur, Yigit},
eprint = {1905.11741v1},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Arvanitakis, Zaidi, Ugur - Unknown - Variational Information Bottleneck for Unsupervised Clustering Deep Gaussian Mixture Embedding.pdf:pdf},
title = {{Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding}},
url = {https://arxiv.org/pdf/1905.11741.pdf}
}
@inproceedings{Kester2010,
abstract = {Rare isotope beam (RIB) accelerator facilities provide rich research opportunities in nuclear physics in particular for nuclear structure physics, nuclear astrophysics and applied physics. The National Superconducting Cyclotron Laboratory (NSCL) at Michigan State University (MSU) is constructing a RIB facility, called 'ReA3'. The facility will provide unique low-energy rare isotope beams by stopping RIBs produced in-flight and reaccelerating them in a compact linac. ReA3 comprises gas stopper systems, an Electron Beam Ion Trap (EBIT) charge state booster, a room temperature radio frequency quadrupole (RFQ), a linac using superconducting quarter wave resonators (QWRs) and an achromatic beam transport and distribution line to the new experimental area. Beams from ReA3 will range from 3 MeV/u for heavy ions to about 6 MeV/u for light ions, as the charge state of the ions can be adjusted by the EBIT. ReA3 will initially use beams from NSCL's Coupled Cyclotron Facility (CCF). Later ReA3 will provide reacceleration capability for the Facility for Rare Isotope Beams (FRIB), a new national user facility funded by the Department of Energy (DOE) that will be hosted at MSU. The ReA3 concept and status of ReA3 will be presented, with emphasis on the commissioning of the facility, which is underway.},
address = {Tsukuba, Japan},
author = {Kester, O and Bazin, D and Benatti, C and Bierwagen, J and Bollen, G and Bricker, S and Crawford, A C and Chouhan, S and Compton, C and Davidson, K and Delauter, J and Doleans, M and Dubbs, L and Elliott, K and Lapierre, A and Hartung, W and Johnson, M and Krause, S and Marti, F and Ottarson, J and Perdikakis, G and Popielarski, L and Popielarski, J and Portillo, M and Rencsok, R and Sanderson, D and Schwarz, S and Verhanovitz, N and Vincent, J and Wlodarczak, J and Wu, X and Yurkon, J and Zeller, A and Zhao, Q and Schempp, A and Schmidt, J},
booktitle = {Proceedings of Linear Accelerator Conference},
file = {::},
pages = {26},
title = {{ReA3-The rare isotope reaccellerator at MSU}},
url = {http://accelconf.web.cern.ch/AccelConf/LINAC2010/papers/mo203.pdf},
year = {2010}
}
@article{Wang2018,
abstract = {We show that faces contain much more information about sexual orientation than can be perceived or interpreted by the human brain. We used deep neural networks to extract features from 35,326 facial images. These features were entered into a logistic regression aimed at classifying sexual orientation. Given a single facial image, a classifier could correctly distinguish between gay and heterosexual men in 81{\%} of cases, and in 71{\%} of cases for women. Human judges achieved much lower accuracy: 61{\%} for men and 54{\%} for women. The accuracy of the algorithm increased to 91{\%} and 83{\%}, respectively, given five facial images per person. Facial features employed by the classifier included both fixed (e.g., nose shape) and transient facial features (e.g., grooming style). Consistent with the prenatal hormone theory of sexual orientation, gay men and women tended to have gender-atypical facial morphology, expression, and grooming styles. Prediction models aimed at gender alone allowed for detecting gay males with 57{\%} accuracy and gay females with 58{\%} accuracy. Those findings advance our understanding of the origins of sexual orientation and the limits of human perception. Additionally, given that companies and governments are increasingly using computer vision algorithms to detect people's intimate traits, our findings expose a threat to the privacy and safety of gay men and women.},
author = {Wang, Yilun and Kosinski, Michal},
doi = {10.1037/pspa0000098},
journal = {Journal of Personality and Social Psychology},
keywords = {Computational social science,Facial morphology,Prenatal hormone theory,Privacy,Sexual orientation},
pages = {246},
publisher = {American Psychological Association Inc.},
title = {{Deep neural networks are more accurate than humans at detecting sexual orientation from facial images}},
volume = {114},
year = {2018}
}
@phdthesis{Treider2018,
abstract = {In this thesis, we review the process of using artificial neural networks as hyperpotential energy surfaces in molecular dynamics simulations, with thorough focus on creation, training and validation. We also outline a more general machine learning process, with emphasis on choosing an atomic environment descriptor together with a regression model. The molecular dynamics library LAMMPS is extended with a minimal-working example of an artificial neural network potential with support for monatomic systems, where long-range interactions are not present. The potential is pre-trained using TensorFlow, a powerful machine learning library from Google, on atomic configurations computed with classical potentials. Our results for bulk-silicon compare nicely with the literature, and encourages future use where the atomic configurations are handled using quantum mechanics, for example by density functional theory. This represents an entirely new way of running ab initio molecular dynamics simulations, with unprecedented computational efficiency.},
author = {Treider, H{\aa}kon Vik{\o}r},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Treider - 2018 - SPEEDING UP AB-INITIO MOLECULAR DYNAMICS WITH ARTIFICIAL NEURAL NETWORKS.pdf:pdf},
pages = {161},
school = {University of Oslo},
title = {{SPEEDING UP AB-INITIO MOLECULAR DYNAMICS WITH ARTIFICIAL NEURAL NETWORKS}},
type = {Master thesis},
url = {https://www.duo.uio.no/handle/10852/59997 http://urn.nb.no/URN:NBN:no-62638},
year = {2018}
}
@incollection{Guo2017,
abstract = {Deep clustering utilizes deep neural networks to learn feature representation that is suitable for clustering tasks. Though demonstrating promising performance in various applications, we observe that existing deep clustering algorithms either do not well take advantage of convolutional neural networks or do not considerably preserve the local structure of data generating distribution in the learned feature space. To address this issue, we propose a deep convolutional embedded clustering algorithm in this paper. Specifically, we develop a convolutional autoencoders structure to learn embedded features in an end-to-end way. Then, a clustering oriented loss is directly built on embedded features to jointly perform feature refinement and cluster assignment. To avoid feature space being distorted by the clustering loss, we keep the decoder remained which can preserve local structure of data in feature space. In sum, we simultaneously minimize the reconstruction loss of convolutional autoencoders and the clustering loss. The resultant optimization problem can be effectively solved by mini-batch stochastic gradient descent and back-propagation. Experiments on benchmark datasets empirically validate the power of convolutional autoencoders for feature learning and the effectiveness of local structure preservation.},
author = {Guo, Xifeng and Liu, Xinwang and Zhu, En and Yin, Jianping},
booktitle = {neural information processing systems},
doi = {10.1007/978-3-319-70096-0_39},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Guo et al. - Unknown - Deep Clustering with Convolutional Autoencoders.pdf:pdf},
keywords = {Convolution-al Neural Networks,Convolutional Autoencoders,Deep Clustering,Unsupervised Learning},
pages = {373},
title = {{Deep Clustering with Convolutional Autoencoders}},
year = {2017}
}
@article{Tan2019,
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4{\%} top-1 / 97.1{\%} top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7{\%}), Flowers (98.8{\%}), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
archivePrefix = {arXiv},
arxivId = {1905.11946},
author = {Tan, Mingxing and Le, Quoc V.},
eprint = {1905.11946},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Tan, Le - 2019 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf:pdf},
month = {may},
title = {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1905.11946},
year = {2019}
}
@techreport{Ruder,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747v2},
author = {Ruder, Sebastian},
eprint = {1609.04747v2},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Ruder - Unknown - An overview of gradient descent optimization algorithms.pdf:pdf},
institution = {Insight Centre for Data Analytics},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://caffe.berkeleyvision.org/tutorial/solver.html},
year = {2016}
}
@article{Stone1974,
abstract = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
archivePrefix = {arXiv},
arxivId = {arXiv:1711.01468v1},
author = {Stone, M.},
doi = {10.2307/2984809},
eprint = {arXiv:1711.01468v1},
journal = {Journal of the Royal Statistical Society},
keywords = {analysis of variance,choice of variables,crossvalidation,doublecross,modelmix,multiple regression,prediction,prescription,univariate estimation},
pages = {111},
pmid = {395},
title = {{Cross-Validatory Choice and Assessment of Statistical Predictions. Journal of the Royal Statistical Societ}},
volume = {36},
year = {1974}
}
@article{McCulloch1943,
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
file = {::},
issn = {0007-4985},
journal = {The Bulletin of Mathematical Biophysics},
month = {dec},
pages = {115},
publisher = {Kluwer Academic Publishers},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {http://link.springer.com/10.1007/BF02478259},
volume = {5},
year = {1943}
}
@inproceedings{tensorflow,
abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition , computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Research, Google},
booktitle = {USENIX Symposium on Operating Systems Design and Implementation},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Abadi et al. - 2015 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
pages = {265},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {https://ai.google/research/pubs/pub45381},
year = {2016}
}
@techreport{python,
author = {van Rossum, Guido and Python development team, The},
file = {::},
title = {{Python Tutorial Release 3.7.0 Guido van Rossum and the Python development team}},
url = {https://bugs.python.org/file47781/Tutorial{\_}EDIT.pdf},
year = {2018}
}
@article{Kullback1951,
author = {Kullback, Solomon. and Leibler, Richard A.},
doi = {10.1214/aoms/1177729694},
journal = {The Annals of Mathematical Statistics},
pages = {79},
publisher = {Institute of Mathematical Statistics},
title = {{On Information and Sufficiency}},
url = {http://projecteuclid.org/euclid.aoms/1177729694},
volume = {22},
year = {1951}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {::},
month = {dec},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@inproceedings{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {International Conference on Learning Representations},
doi = {10.1.1.740.6937},
eprint = {1409.1556},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
year = {2015}
}
@inproceedings{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
booktitle = {International Conference on Learning Representations},
eprint = {1312.6114},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes(3).pdf:pdf},
month = {dec},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114 https://openreview.net/forum?id=33X9fd2-9FyZd},
year = {2014}
}
@techreport{Lipton2015,
abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connec-tionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated groundbreaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
archivePrefix = {arXiv},
arxivId = {1506.00019v4},
author = {Lipton, Zachary C and Berkowitz, John and Elkan, Charles},
eprint = {1506.00019v4},
file = {::},
title = {{A Critical Review of Recurrent Neural Networks for Sequence Learning}},
url = {https://arxiv.org/pdf/1506.00019.pdf},
year = {2015}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
file = {::},
journal = {Communications of the ACM},
keywords = {Issue 55.10,abstract image},
pages = {78},
title = {{A few useful things to know about machine learning}},
url = {https://homes.cs.washington.edu/{~}pedrod/papers/cacm12.pdf http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Jolliffe1982,
abstract = {The use of principal components in regression has received a lot of attention in the literature in the past few years, and the topic is now beginning to appear in textbooks. Along with the use of principal component regression there appears to have been a growth in the misconception that the principal components with small eigenvalues will very rarely be of any use in regression. The purpose of this note is to demonstrate that these components can be as important as those with large variance. This is illustrated with four examples, three of which have already appeared in the literature. CR - Copyright {\&}{\#}169; 1982 Royal Statistical Society},
author = {Jolliffe, Ian T.},
doi = {10.2307/2348005},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Jolliffe - 1982 - A Note on the Use of Principal Components in Regression.pdf:pdf},
journal = {Applied Statistics},
pages = {300},
title = {{A Note on the Use of Principal Components in Regression}},
url = {http://automatica.dei.unipd.it/public/Schenato/PSC/2010{\_}2011/gruppo4-Building{\_}termo{\_}identification/IdentificazioneTermodinamica20072008/Biblio/Articoli/PCR vecchio 82.pdf},
volume = {31},
year = {1982}
}
@article{Scholkopf1996,
abstract = {We describe a new method for performing a nonlinear form of Principal Component Analysis. By the use of integral operator kernel functions, we can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible 5-pixel products in 16x16 images. We give the derivation of the method, along with a discussion of other techniques which can be made nonlinear with the kernel approach; and present first experimental results on nonlinear feature extraction for pattern recognition.},
author = {Sch{\"{o}}lkopf, Bernhard and Smola, Alexander and M{\"{u}}ller, Klaus-Robert},
doi = {10.1.1.100.3636},
number = {44},
pages = {1299--1319},
title = {{Component Analysis as a Kernel Eigenvalue Problem}},
url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.100.3636 http://www.face-rec.org/algorithms/Kernel/kernelPCA{\_}scholkopf.pdf},
volume = {5},
year = {1996}
}
@article{Newman1972,
abstract = {Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. It also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency.},
author = {Newman, W and Duda, Richard O and Hart, Peter E},
file = {::},
journal = {Communications of the ACM},
keywords = {Hough transformation CR Categories: 363,and Phrases: picture processing,colinear points,curve detection,line detection,pattern recognition,point-line transformation},
pages = {11},
title = {{Graphics and Use of the Hough Transformation To Detect Lines and Curves in Pictures}},
volume = {15},
year = {1972}
}
@incollection{Aßfalg2006,
author = {A{\ss}falg, Johannes and Kriegel, Hans-Peter and Kr{\"{o}}ger, Peer and Kunath, Peter and Pryakhin, Alexey and Renz, Matthias},
doi = {10.1007/11687238_19},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/A{\ss}falg et al. - 2006 - Similarity Search on Time Series Based on Threshold Queries(2).pdf:pdf},
pages = {276--294},
publisher = {Springer, Berlin, Heidelberg},
title = {{Similarity Search on Time Series Based on Threshold Queries}},
url = {http://link.springer.com/10.1007/11687238{\_}19},
year = {2006}
}
@book{Polk2002,
abstract = {"A Bradford book." Annotation Computational modeling plays a central role in cognitive science. This book provides a comprehensive introduction to computational models of human cognition. It covers major approaches and architectures, both neural network and symbolic; major theoretical issues; and specific computational models of a variety of cognitive processes, ranging from low-level (e.g., attention and memory) to higher-level (e.g., language and reasoning). The articles included in the book provide original descriptions of developments in the field. The emphasis is on implemented computational models rather than on mathematical or nonformal approaches, and on modeling empirical data from human subjects. 1. Role of knowledge in discourse comprehension: a construction-integration model / Walter A. Kintssch -- 2. Act: a simple theory of complex cognition / John R. Anderson -- 3. Preliminary analysis of the soar architecture as a basis for general intelligence / Paul S. Rosenbloom, John E. Laird, Allen Newell, and Robert McCarl -- 4. Adaptive executive control: flexible multiple-task performance without pervasive immutable response-selection bottlenecks / David E. Meyer [and others] -- 5. Capacity theory of comprehension: individual differences in working memory / Marcel A. Just and Patricia A. Carpenter -- 6. How neural networks learn from experience / Geoffrey E. Hinton -- 7. Hopfield model / John Hertz, Anders Krogh, and Richard G. Palmer -- 8. Learning representations by back-propagating errors / David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams -- 9. Forward models: supervised learning with a distal teacher / Michael I. Jordan and David E. Rumelhart -- 10. Finding structure in Time / Jeffrey L. Elman -- 11. Self-organizing neural network for supervised learning, recognition, and prediction / Gail A. Carpenter and Stephen Grossberg -- 12. Optimality: from neural networks to universal grammar / Alan Prince and Paul Smolensky -- 13. Dynamic binding in a neural network for shape recognition / John E. Hummel and Irving Biederman -- 15. End of the line for a brain-damaged model of unilateral neglect / Michael C. Mozer, Peter W. Halligan, and John C. Marshall -- 16. Integrated theory of list memory / John R. Anderson, Dan Bothell, Christian Lebiere, and Michael Matessa -- 17. Why there are complementary learning systems in hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory / James L. McClelland, Bruce L. McNaughton, and Randall C. O'Reilly -- 18. ALCOVE: an exemplar-based connectionist model of category learning / John K. Kruschke -- 19. How people learn to skip steps / Stephen B. Blessing and John R. Anderson -- 20. Acquisition of children's addition strategies: a model of impasse-free, knowledge-level learning / Randolph M. Jones and Kurt Van Lehn -- 21. Learning from a connectionist model of the acquisition of the English past tense / Kim Plunkett and Virginia A. Marchman -- 22. Acquiring the mapping from meaning to sounds / Garrison W. Cottrell and Kim plunkett -- 23. Understanding normal and impaired word reading: computational principles in quasi-regular domains / David C. Plaut, James L. McClelland, Mark S. Seidenberg, and Karalyn Patterson -- 24. Language production and serial order: a functional analysis and a model / Gary S. Dell, Lisa K. Burger, and William R. Svec -- 25. Interference in short-term memory: the magical number two (or three) in sentence processing / Richard L. Lewis -- 26. Similarity, interactive activation, and mapping: an overview / Robert L. Goldstone and Douglas L. Medin -- 27. Analogical mapping by constraint satisfaction / Keith J. Holyoak and Paul Thagard -- 28. MAC/FAC: a model of similarity-based retrieval / Kenneth D. Forbus, Dedre Gentner, and Keith Law -- 29. Distributed representations of structure: a theory of analogical access and mapping / John E. Hummel and Keith J. Holyoak -- 30. Case-based learning: predictive features in indexing / Colleen M. Seifert [and others] -- 31. Feature-based induction / Steven A. Sloman -- 32. Deduction as verbal reasoning / Thad A. Polk and Allen Newell -- 33. Project Ernestine: validating a GOMS analysis for predicting and explaining real-world task performance / Wayne D. Gray, Bonnie E. John, and Michael E. Atwood -- 34. Connectionism and the problem of systematicity (continued): why Smolensky's solution still doesn't work / Jerry Fodor -- 35. Networks and theories: the place of connectionism in cognitive science / Michael C. McCloskey -- 36. Neuropsychological inference with an interactive brain: a critique of the "locality" assumption / Martha J. Farah -- 37. Is human cognition adaptive? / John R. Anderson -- 38. Précis of Unified Theories of Cognition / Allen Newell.},
author = {Polk, Thad A. and Seifert, Colleen M.},
isbn = {0262661160},
pages = {1270},
publisher = {MIT Press},
title = {{Cognitive modeling}},
url = {https://books.google.no/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=learning+representations+by+back+propagating+errors{\&}ots=z{\_}Hk2kK{\_}QR{\&}sig=W9viwB1N834xqMygwHJCUIWVkmQ{\&}redir{\_}esc=y{\#}v=onepage{\&}q=learning representations by back propagating errors{\&}f=fal},
year = {2002}
}
@article{Acciarri2017a,
abstract = {We present several studies of convolutional neural networks applied to data coming from the MicroBooNE detector, a liquid argon time projection chamber (LArTPC). The algorithms studied include the classification of single particle images, the localization of single particle and neutrino interactions in an image, and the detection of a simulated neutrino event overlaid with cosmic ray backgrounds taken from real detector data. These studies demonstrate the potential of convolutional neural networks for particle identification or event detection on simulated neutrino interactions. We also address technical issues that arise when applying this technique to data from a large LArTPC at or near ground level.},
archivePrefix = {arXiv},
arxivId = {1611.05531},
author = {Acciarri, R. and Adams, C. and An, R. and Asaadi, J. and Auger, M. and Bagby, L. and Baller, B. and Barr, G. and Bass, M. and Bay, F. and Bishai, M. and Blake, A. and Bolton, T. and Bugel, L. and Camilleri, L. and Caratelli, D. and Carls, B. and Fernandez, R. Castillo and Cavanna, F. and Chen, H. and Church, E. and Cianci, D. and Collin, G. H. and Conrad, J. M. and Convery, M. and Crespo-Anad{\'{o}}n, J. I. and {Del Tutto}, M. and Devitt, D. and Dytman, S. and Eberly, B. and Ereditato, A. and Sanchez, L. Escudero and Esquivel, J. and Fleming, B. T. and Foreman, W. and Furmanski, A. P. and Garvey, G. T. and Genty, V. and Goeldi, D. and Gollapinni, S. and Graf, N. and Gramellini, E. and Greenlee, H. and Grosso, R. and Guenette, R. and Hackenburg, A. and Hamilton, P. and Hen, O. and Hewes, J. and Hill, C. and Ho, J. and Horton-Smith, G. and James, C. and {De Vries}, J. Jan and Jen, C. M. and Jiang, L. and Johnson, R. A. and Jones, B. J.P. and Joshi, J. and Jostlein, H. and Kaleko, D. and Karagiorgi, G. and Ketchum, W. and Kirby, B. and Kirby, M. and Kobilarcik, T. and Kreslo, I. and Laube, A. and Li, Y. and Lister, A. and Littlejohn, B. R. and Lockwitz, S. and Lorca, D. and Louis, W. C. and Luethi, M. and Lundberg, B. and Luo, X. and Marchionni, A. and Mariani, C. and Marshall, J. and Caicedo, D.A. A.Martinez and Meddage, V. and Miceli, T. and Mills, G. B. and Moon, J. and Mooney, M. and Moore, C. D. and Mousseau, J. and Murrells, R. and Naples, D. and Nienaber, P. and Nowak, J. and Palamara, O. and Paolone, V. and Papavassiliou, V. and Pate, S. F. and Pavlovic, Z. and Porzio, D. and Pulliam, G. and Qian, X. and Raaf, J. L. and Rafique, A. and Rochester, L. and {Von Rohr}, C. Rudolf and Russell, B. and Schmitz, D. W. and Schukraft, A. and Seligman, W. and Shaevitz, M. H. and Sinclair, J. and Snider, E. L. and Soderberg, M. and S{\"{o}}ldner-Rembold, S. and Soleti, S. R. and Spentzouris, P. and Spitz, J. and John, J. and Strauss, T. and Szelc, A. M. and Tagg, N. and Terao, K. and Thomson, M. and Toups, M. and Tsai, Y. T. and Tufanli, S. and Usher, T. and {Van De Water}, R. G. and Viren, B. and Weber, M. and Weston, J. and Wickremasinghe, D. A. and Wolbers, S. and Wongjirad, T. and Woodruff, K. and Yang, T. and Zeller, G. P. and Zennamo, J. and Zhang, C.},
doi = {10.1088/1748-0221/12/03/P03011},
eprint = {1611.05531},
issn = {17480221},
journal = {Journal of Instrumentation},
keywords = {Analysis and statistical methods,Image filtering,Particle identification methods,Time projection chambers},
month = {mar},
pages = {P03011--P03011},
publisher = {IOP Publishing},
title = {{Convolutional neural networks applied to neutrino events in a liquid argon time projection chamber}},
url = {http://stacks.iop.org/1748-0221/12/i=03/a=P03011?key=crossref.b0d7d7efe193e427b920796770b371c9},
volume = {12},
year = {2017}
}
@article{Lin2017,
abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through "cheap learning" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various "no-flattening theorems" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that {\$}n{\$} variables cannot be multiplied using fewer than 2{\^{}}n neurons in a single hidden layer.},
author = {Lin, Henry W. and Tegmark, Max and Rolnick, David},
doi = {10.1007/s10955-017-1836-5},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Tegmark, Rolnick - 2017 - Why Does Deep and Cheap Learning Work So Well.pdf:pdf},
journal = {Journal of Statistical Physics},
keywords = {Artificial neural networks,Deep learning,Statistical physics},
pages = {1223},
publisher = {Springer US},
title = {{Why Does Deep and Cheap Learning Work So Well?}},
volume = {168},
year = {2017}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980v9},
author = {Kingma, Diederik P and {Lei Ba}, Jimmy},
booktitle = {International Conference on Learning Representations},
eprint = {1412.6980v9},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Lei Ba - 2015 - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.pdf:pdf},
title = {{ADAM: a method for stochastic optimization}},
url = {https://arxiv.org/pdf/1412.6980.pdf https://openreview.net/forum?id=33X9fd2-9FyZd{\&}noteId=33X9fd2-9FyZd},
year = {2014}
}
@article{Mittig2015,
abstract = {Reaccelerated radioactive beams near the Coulomb barrier, which are starting to be available from the ReA3 accelerator at NSCL and in next future at FRIB, will open up new opportunities for the study of nuclear structure near the drip lines. Efficient measurement techniques must be developed to compensate for the limited intensity of the most exotic beams. The Active-Target Time Projection Chamber (AT-TPC) constructed at MSU solves this problem by providing the increased luminosity of a thick target while maintaining a good energy resolution by tracking the reaction vertex over an essentially 4$\pi$ solid angle. The AT-TPC and similar detectors allow us to take full advantage of the radioactive ion beams at present and future nuclear physics facilities to explore the frontier of rare isotopes.},
author = {Mittig, Wolfgang and Beceiro-Novo, Saul and Fritsch, Adam and Abu-Nimeh, Faisal and Bazin, Daniel and Ahn, Tan and Lynch, William G. and Montes, Fernando and Shore, Amiee and Suzuki, D. and Usher, N. and Yurkon, J. and Kolata, J.J. and Howard, A. and Roberts, A.L. and Tang, X.D. and Becchetti, F.D.},
doi = {10.1016/J.NIMA.2014.10.048},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Mittig et al. - 2015 - Active Target detectors for studies with exotic beams Present and next future(2).pdf:pdf},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
pages = {494},
publisher = {North-Holland},
title = {{Active Target detectors for studies with exotic beams: Present and next future}},
url = {https://www.sciencedirect.com/science/article/pii/S0168900214012054},
volume = {784},
year = {2015}
}
@inproceedings{Jiang2017a,
abstract = {Clustering is among the most fundamental tasks in computer vision and machine learning. In this paper, we propose Variational Deep Embedding (VaDE), a novel unsupervised generative clustering approach within the framework of Variational Auto-Encoder (VAE). Specifically, VaDE models the data generative procedure with a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1) the GMM picks a cluster; 2) from which a latent embedding is generated; 3) then the DNN decodes the latent embedding into observables. Inference in VaDE is done in a variational way: a different DNN is used to encode observables to latent embeddings, so that the evidence lower bound (ELBO) can be optimized using Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameterization trick. Quantitative comparisons with strong baselines are included in this paper, and experimental results show that VaDE significantly outperforms the state-of-the-art clustering methods on 4 benchmarks from various modalities. Moreover, by VaDE's generative nature, we show its capability of generating highly realistic samples for any specified cluster, without using supervised information during training. Lastly, VaDE is a flexible and extensible framework for unsupervised generative clustering, more general mixture models than GMM can be easily plugged in.},
author = {Jiang, Zhuxi and Zheng, Yin and Tan, Huachun and Tang, Bangsheng and Zhou, Hanning},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
isbn = {9780999241103},
title = {{Variational deep embedding: An unsupervised generative approach to Clustering}},
year = {2017}
}
@techreport{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
booktitle = {Journal of Machine Learning Research},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf},
volume = {15},
year = {2014}
}
@article{Silver2018,
abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
doi = {10.1126/science.aar6404},
file = {::},
issn = {10959203},
journal = {Science},
pages = {1140},
publisher = {American Association for the Advancement of Science},
title = {{A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play}},
volume = {362},
year = {2018}
}
@techreport{Hayes1967,
abstract = {Sonic boom theory /review/},
author = {Hayes, W. D.},
booktitle = {Sonic Boom Research},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Hayes - 1967 - Brief review of the basic theory.pdf:pdf},
keywords = {sonic boom},
pages = {3},
title = {{Brief review of the basic theory}},
url = {https://ocw.mit.edu/courses/nuclear-engineering/22-106-neutron-interactions-and-applications-spring-2010/lecture-notes/MIT22{\_}106S10{\_}lec04b.pdf},
volume = {NASA SP-14},
year = {1967}
}
@techreport{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {Bergstra, James and Bengio, Yoshua},
booktitle = {Journal of Machine Learning Research},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Bergstra, Bengio - 2012 - Random Search for Hyper-Parameter Optimization Yoshua Bengio.pdf:pdf},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281},
title = {{Random Search for Hyper-Parameter Optimization Yoshua Bengio}},
url = {http://scikit-learn.sourceforge.net.},
volume = {13},
year = {2012}
}
@INPROCEEDINGS{Ester96adensity-based,
    author = {Martin Ester and Hans-Peter Kriegel and Jörg Sander and Xiaowei Xu},
    title = {A density-based algorithm for discovering clusters in large spatial databases with noise},
    booktitle = {},
    year = {1996},
    pages = {226--231},
    publisher = {AAAI Press}
}
@misc{Karpathy2015,
abstract = {There's something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience I've in fact reached the opposite conclusion). Fast forward about a year: I'm training RNNs all the time and I've witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you.},
author = {Karpathy, Andrej},
title = {{The Unreasonable Effectiveness of Recurrent Neural Networks}},
url = {https://karpathy.github.io/2015/05/21/rnn-effectiveness/},
urldate = {2019-07-22},
year = {2015}
}
@inproceedings{Jiang2017,
abstract = {Clustering is among the most fundamental tasks in machine learning and artificial intelligence. In this paper, we propose Variational Deep Embedding (VaDE), a novel unsupervised generative clustering approach within the framework of Varia-tional Auto-Encoder (VAE). Specifically, VaDE models the data generative procedure with a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1) the GMM picks a cluster; 2) from which a latent embedding is generated; 3) then the DNN decodes the latent embedding into an observable. Inference in VaDE is done in a variational way: a different DNN is used to encode observables to latent embeddings, so that the evidence lower bound (ELBO) can be optimized using the Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameteriza-tion trick. Quantitative comparisons with strong baselines are included in this paper , and experimental results show that VaDE significantly outperforms the state-of-the-art clustering methods on 5 benchmarks from various modalities. Moreover, by VaDE's generative nature, we show its capability of generating highly realistic samples for any specified cluster, without using supervised information during training.},
archivePrefix = {arXiv},
arxivId = {1611.05148v3},
author = {Jiang, Zhuxi and Zheng, Yin and Tan, Huachun and Tang, Bangsheng and Zhou, Hanning},
eprint = {1611.05148v3},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Jiang et al. - 2017 - Variational Deep Embedding An Unsupervised and Generative Approach to Clustering.pdf:pdf},
title = {{Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering}},
url = {http://ijcai-17.org/accepted-papers.html},
year = {2017}
}
@article{Kuchera2019,
abstract = {We evaluate machine learning methods for event classification in the Active-Target Time Projection Chamber detector at the National Superconducting Cyclotron Laboratory (NSCL) at Michigan State University. Currently, events of interest are selected via cuts in the track fitting stage of the analysis workflow. An explicit classification step to single out the desired reaction product would result in more accurate physics results as well as a faster analysis process. We tested binary and multi-class classification methods on data produced by the 46Ar(p,p) experiment run at the NSCL in September 2015. We found that fine-tuning a pre-trained convolutional neural network produced the most successful classifier of proton scattering events in the experimental data, when trained on both experimental and simulated data. We present results from this investigation and conclude with recommendations for event classification in future experiments.},
author = {Kuchera, Michelle. P. and Ramanujan, Raghu and Taylor, Jack Z. and Strauss, Ryan R. and Bazin, Daniel and Bradt, Joshua W. and Chen, Ruiming},
doi = {10.1016/j.nima.2019.05.097},
file = {:Users/solli/Downloads/kuchera{\_}et{\_}al.pdf:pdf},
journal = {Nuclear Instruments and Methods in Physics Research, Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
keywords = {Active targets,Classification,Machine learning,Neural networks,Time projection chamber},
pages = {156},
publisher = {North-Holland},
title = {{Machine learning methods for track classification in the AT-TPC}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0168900219308046},
volume = {940},
year = {2019}
}
@techreport{Xie2016,
abstract = {Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1511.06335},
author = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
eprint = {1511.06335},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Xie, Girshick, Farhadi - 2016 - Unsupervised Deep Embedding for Clustering Analysis.pdf:pdf},
title = {{Unsupervised Deep Embedding for Clustering Analysis}},
url = {http://arxiv.org/abs/1511.06335},
year = {2015}
}
@phdthesis{Bradt2017,
abstract = {While the nuclear shell model accurately describes the structure of nuclei near stability, the structure of unstable, neutron-rich nuclei is still an area of active research. One region of interest is the set of nuclei near N = 28. The shell model suggests that these nuclei should be approximately spherical due to the shell gap predicted by their magic number ofneutrons; however, experiments have shown that the nuclei in this region rapidly become deformed as protons are removed from the spherical 48Ca. This makes 46Ar a particularly interesting system as it lies in a transition region between 48Ca and lighter isotones that are known to be deformed. An experiment was performed at the National Superconducting Cyclotron Laboratory (NSCL) to measure resonant proton scattering on 46Ar. The resonances observed in this reaction correspond to unbound levels in the 47K intermediate state nucleus which are isobaric analogues of states in the 47Ar nucleus. By measuring the spectroscopic factors of these states in 47Ar, we gain information about the single-particle structure of this system, which is directly related to the size of the N= 28 shell gap. Four resonances were observed: one corresponding to the ground state in 47Ar, one corresponding its first excited 1/2− state, and two corresponding to 1/2+ states in either 47Ar or the intermediate state nucleus. However, only a limited amount of information about these states could be recovered due to the low experimental statistics and limited angular resolution caused by pileup rejection and the inability to ac- curately reconstruct the beam particle track. In addition to the nuclear physics motivations, this experiment served as the radioactive beam com- missioning for the Active-Target Time Projection Chamber (AT-TPC). The AT-TPC is a new gas-filled charged particle detector built at the NSCL to measure low-energy radioactive beams from the ReA3 facility. Since the gas inside the detector serves as both the tracking medium and the scattering target, reactions are measured over a continuous range of energies with near-4$\pi$ solid angle coverage. This ex- periment demonstrated that tracks recorded by the AT-TPC can be reconstructed to a good resolution, and it established the feasibility of performing similar experiments with this detector in the future},
author = {Bradt, Joshua William},
file = {:Users/solli/Downloads/bradt-thesis.pdf:pdf},
school = {Michigan State University},
title = {{Measurement of isobaric analogue resonances of 47Ar with the active target time projection chamber}},
year = {2017}
}
@book{Burnham2002,
abstract = {2nd ed. Revised edition of: Model selection and inference. c1998. This book is unique in that it covers the philosophy of model-based data analysis and a strategy for the analysis of empirical data. The book introduces information theoretic approaches and focuses critical attention on a priori modeling and the selection of a good approximating model that best represents the inference supported by the data. Kullback-Leibler Information represents a fundamental quantity in science and is Hirotugu Akaike's basis for model selection. The maximized log-likelihood function can be bias-corrected to provide an estimate of expected, relative Kullback-Leibler information. This leads to Akaike's Information Criterion (AIC) and various extensions. These are relatively simple and easy to use in practice. The information theoretic approaches provide a unified and rigorous theory, an extension of likelihood theory, an important application of information theory, and are objective and practical to employ across a very wide class of empirical problems. Model selection, under the information theoretic approach presented here, attempts to identify the (likely) best model, orders the models from best to worst, and measures the plausibility ("calibration") that each model is really the best as an inference. Model selection methods are extended to allow inference from more than a single "best" model. The book presents several new approaches to estimating model selection uncertainty and incorporating selection uncertainty into estimates of precision. An array of examples is given to illustrate various technical issues. This is an applied book written primarily for biologists and statisticians using models for making inferences from empirical data. People interested in the empirical sciences will find this material useful as it offers an alternative to hypothesis testing and Bayesian. 1. Introduction -- 2. Information and likelihood theory: a basis for model selection and inference -- 3. Basic use of the information-theoretic approach -- 4. Formal inference from more than one model: multimodel inference (MMI) -- 5. Monte Carlo insights and extended examples -- 6. Advanced issues and deeper insights -- 7. Statistical theory and numerical results -- 8. Summary.},
author = {Burnham, Kenneth P. and Anderson, David Raymond and Burnham, Kenneth P.},
isbn = {0387953647},
pages = {488},
publisher = {Springer},
title = {{Model selection and multimodel inference : a practical information-theoretic approach}},
url = {https://books.google.no/books?id=fT1Iu-h6E-oC{\&}pg=PA51{\&}redir{\_}esc=y{\#}v=onepage{\&}q{\&}f=false},
year = {2002}
}
@inproceedings{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {neural information processing systems},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
pages = {1097},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},
year = {2012}
}
@inproceedings{Keskar2016,
abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say {\$}32{\$}-{\$}512{\$} data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
archivePrefix = {arXiv},
arxivId = {1609.04836},
author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
booktitle = {International Conference on Learning Representations},
eprint = {1609.04836},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Keskar et al. - 2016 - On Large-Batch Training for Deep Learning Generalization Gap and Sharp Minima.pdf:pdf},
title = {{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}},
url = {https://openreview.net/forum?id=H1oyRlYgg},
year = {2017}
}
@inproceedings{Higgins2017,
abstract = {Learning an interpretable factorised representation of the independent data gen-erative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce $\beta$-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hy-perparameter $\beta$ that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that $\beta$-VAE with appropriately tuned $\beta$ {\textgreater} 1 qualitatively outperforms VAE ($\beta$ = 1), as well as state of the art unsu-pervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, $\beta$-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter $\beta$, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander and Deepmind, Google},
booktitle = {International Conference on Learning Representationsr},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Higgins et al. - Unknown - $\beta$-VAE LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK.pdf:pdf},
title = {{$\beta$-VAE: Learning basic visual concepts with a constrained variational framework}},
url = {https://pdfs.semanticscholar.org/a902/26c41b79f8b06007609f39f82757073641e2.pdf https://openreview.net/forum?id=Sy2fzU9gl},
year = {2017}
}
@techreport{Sutskever2013,
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter , it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initial-izations have likely failed due to poor ini-tialization schemes. Furthermore, carefully tuned momentum methods suce for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
file = {::},
title = {{On the importance of initialization and momentum in deep learning}},
url = {http://proceedings.mlr.press/v28/sutskever13.pdf},
year = {2013}
}
@article{Odena2016,
author = {Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
doi = {10.23915/distill.00003},
journal = {Distill},
number = {10},
pages = {1},
title = {{Deconvolution and Checkerboard Artifacts}},
url = {http://distill.pub/2016/deconv-checkerboard},
volume = {1},
year = {2016}
}
@article{Fort2019,
archivePrefix = {arXiv},
arxivId = {1901.09491},
author = {Fort, Stanislav and Nowak, Pawe{\l} Krzysztof and Narayanan, Srini},
eprint = {1901.09491},
file = {::},
month = {jan},
title = {{Stiffness: A New Perspective on Generalization in Neural Networks}},
url = {https://arxiv.org/abs/1901.09491},
year = {2019}
}
@inproceedings{Gregor2015,
abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
archivePrefix = {arXiv},
arxivId = {1502.04623v2},
author = {Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
booktitle = {International conference on machine learning},
eprint = {1502.04623v2},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Gregor et al. - Unknown - DRAW A Recurrent Neural Network For Image Generation Ivo Danihelka.pdf:pdf},
pages = {1462},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
volume = {37},
year = {2015}
}
@techreport{Szegedy2014,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception , which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842v1},
author = {Szegedy, Christian and Liu, Wei and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
eprint = {1409.4842v1},
file = {::},
title = {{Going deeper with convolutions}},
url = {https://arxiv.org/pdf/1409.4842.pdf},
year = {2014}
}
@phdthesis{Taylor208,
author = {Taylor, Jack Ziegler},
booktitle = {Thesis},
file = {:Users/solli/Downloads/taylor-thesis.pdf:pdf},
school = {Davidson College},
title = {{EVALUATING MACHINE LEARNING METHODS FOR EVENT CLASSIFICATION IN THE ACTIVE-TARGET TIME PROJECTION CHAMBER}},
type = {Undergraduate honors thesis},
year = {2018}
}
@inproceedings{Zhao,
abstract = {A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized inference distributions and, in some cases, improving the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distribution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (InfoVAE) that mitigate these problems. We show that our model can significantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decoding distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics.},
archivePrefix = {arXiv},
arxivId = {1706.02262},
author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
booktitle = {International conference on machine learning},
eprint = {1706.02262},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Zhao, Song, Ermon - Unknown - InfoVAE Balancing Learning and Inference in Variational Autoencoders.pdf:pdf},
pages = {24},
title = {{InfoVAE: Information Maximizing Variational Autoencoders}},
url = {http://arxiv.org/abs/1706.02262},
year = {2018}
}
@misc{Sonzogni2019,
author = {Sonzogni, Alejandro},
title = {{NuDat 2.7}},
url = {https://www.nndc.bnl.gov/nudat2/reSize.jsp?cc=5},
urldate = {2019-07-30},
year = {2019}
}
@book{Neyman1967,
abstract = {Editor: 1945/46- J. Neyman.},
author = {Wishart, J. and Neyman, Jerzy},
booktitle = {The Mathematical Gazette},
doi = {10.2307/3610901},
file = {::},
pages = {71},
publisher = {University of California Press},
title = {{Proceedings of the Berkeley Symposium on Mathematical Statistics and Probability}},
url = {https://www.jstor.org/stable/3610901?origin=crossref},
volume = {34},
year = {1950}
}
@article{Seybold2019,
abstract = {Variational autoencoders learn unsupervised data representations, but these models frequently converge to minima that fail to preserve meaningful semantic information. For example, variational autoencoders with autoregressive decoders often collapse into autodecoders, where they learn to ignore the encoder input. In this work, we demonstrate that adding an auxiliary decoder to regularize the latent space can prevent this collapse, but successful auxiliary decoding tasks are domain dependent. Auxiliary decoders can increase the amount of semantic information encoded in the latent space and visible in the reconstructions. The semantic information in the variational autoencoder's representation is only weakly correlated with its rate, distortion, or evidence lower bound. Compared to other popular strategies that modify the training objective, our regularization of the latent space generally increased the semantic information content.},
archivePrefix = {arXiv},
arxivId = {1905.07478},
author = {Seybold, Bryan and Fertig, Emily and Alemi, Alex and Fischer, Ian},
eprint = {1905.07478},
month = {may},
title = {{Dueling Decoders: Regularizing Variational Autoencoder Latent Spaces}},
url = {http://arxiv.org/abs/1905.07478},
year = {2019}
}
@article{Bradt2017a,
abstract = {The Active-Target Time Projection Chamber (AT-TPC) was recently built and commissioned at the National Superconducting Cyclotron Laboratory at Michigan State University. This gas-filled detector uses an active-target design where the gas acts as both the tracking medium and the reaction target. Operating inside a 2T solenoidal magnetic field, the AT-TPC records charged particle tracks that can be reconstructed to very good energy and angular resolutions. The near-4$\pi$ solid angle coverage and thick target of the detector are well-suited to experiments with low secondary beam intensities. In this paper, the design and instrumentation of theAT-TPC are described along with the methods used to analyze the data it produces. A simulation of the detector's performance and some results from its commissioning with a radioactive 46Ar beam are also presented.},
author = {Bradt, J. and Bazin, D. and Abu-Nimeh, F. and Ahn, T. and Ayyad, Y. and {Beceiro Novo}, S. and Carpenter, L. and Cortesi, M. and Kuchera, M.P. and Lynch, W.G. and Mittig, W. and Rost, S. and Watwood, N. and Yurkon, J.},
doi = {10.1016/J.NIMA.2017.09.013},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Bradt et al. - 2017 - Commissioning of the Active-Target Time Projection Chamber.pdf:pdf},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
pages = {65},
publisher = {North-Holland},
title = {{Commissioning of the Active-Target Time Projection Chamber}},
url = {https://www.sciencedirect.com/science/article/pii/S0168900217309683},
volume = {875},
year = {2017}
}
@techreport{Tibshirani1996,
abstract = {We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
author = {Tibshirani, Robert},
booktitle = {Source: Journal of the Royal Statistical Society. Series B (Methodological)},
file = {::},
pages = {267},
title = {{Regression Shrinkage and Selection via the Lasso}},
url = {https://www.jstor.org/stable/pdf/2346178.pdf?refreqid=excelsior{\%}3A0665690fe41c338bbaa8d3f1883ccb60},
volume = {58},
year = {1996}
}
@misc{Hjorth-Jensen,
author = {Hjorth-Jensen, Morten},
file = {::},
title = {{Computational Physics 2}},
url = {https://compphysics.github.io/ComputationalPhysics2/doc/web/course},
urldate = {2019-09-23},
year = {2019}
}
@article{Pedregosa2011,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
file = {::},
journal = {Journal of Machine Learning Research},
pages = {2825},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://www.jmlr.org/papers/v12/pedregosa11a.html},
volume = {12},
year = {2011}
}
@inproceedings{Chinchor1992,
author = {Chinchor, Nancy},
booktitle = {Fourth Message Understanding Conference},
file = {::},
pages = {22},
title = {{Evaluation Metrics}},
year = {1992}
}
@article{Yang,
archivePrefix = {arXiv},
arxivId = {1610.04794},
author = {Yang, Bo and Fu, Xiao and Sidiropoulos, Nicholas D and Hong, Mingyi},
eprint = {1610.04794},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2016 - Towards K-means-friendly Spaces Simultaneous Deep Learning and Clustering.pdf:pdf},
month = {oct},
title = {{Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering}},
url = {https://github.com/boyangumn/DCN. http://arxiv.org/abs/1610.04794},
year = {2016}
}
@article{Bradt2018,
journal = {Physics Letters B},
pages = {155},
publisher = {North-Holland},
title = {{Study of spectroscopic factors at N = 29 using isobaric analogue resonances in inverse kinematics}},
url = {https://www.sciencedirect.com/science/article/pii/S0370269318300236?via{\%}3Dihub},
volume = {778},
year = {2018}
}
@inproceedings{Frankle2018,
abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90{\%}, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20{\%} of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
archivePrefix = {arXiv},
arxivId = {1803.03635v5},
author = {Frankle, Jonathan and Carbin, Michael},
booktitle = {International Conference on Learning Representations},
eprint = {1803.03635v5},
file = {::},
title = {{The lottery ticket hypothesis: finding sparse, trainable neural networks}},
url = {https://openreview.net/forum?id=rJl-b3RcF7},
year = {2019}
}
@article{Giomataris1996,
abstract = {We describe a novel structure for a gaseous detector that is under development at Saclay. It consists of a two-stage parallet-plate avalanche chamber of small amplification gap (100 $\mu$m) combined with a conversion-drift space. It follows a fast removal of positive ions produced during the avalanche development. Fast signals (≤1 ns) are obtained during the collection of the electron avalanche on the anode microstrip plane. The positive ion signal has a duration of 100 ns. The fast evacuation of positive ions combined with the high granularity of the detector provide a high rate capability. Gas gains of up to 105 have been achieved.},
author = {Giomataris, Yannis and Rebourgeard, Ph. and Robert, J.P. and Charpak, G.},
doi = {10.1016/0168-9002(96)00175-1},
file = {:Users/solli/Library/Application Support/Mendeley Desktop/Downloaded/Giomataris et al. - 1996 - MICROMEGAS a high-granularity position-sensitive gaseous detector for high particle-flux environments.pdf:pdf},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
pages = {29},
publisher = {North-Holland},
title = {{MICROMEGAS: a high-granularity position-sensitive gaseous detector for high particle-flux environments}},
volume = {376},
year = {1996}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
journal = {Neural Computation},
pages = {1735},
publisher = {MIT Press Journals},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@misc{Jianqiang19,
author = {Jianqiang, Ma},
title = {{All of Recurrent Neural Networks - Jianqiang Ma - Medium}},
url = {https://medium.com/@jianqiangma/all-about-recurrent-neural-networks-9e5ae2936f6e},
urldate = {2019-11-13}
}
@article{numpy,
abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
author = {van der Walt, St{\'{e}}fan and Colbert, S Chris and Varoquaux, Ga{\"{e}}l},
doi = {10.1109/MCSE.2011.37},
journal = {Computing in Science {\&} Engineering},
pages = {22},
title = {{The NumPy Array: A Structure for Efficient Numerical Computation}},
url = {http://ieeexplore.ieee.org/document/5725236/},
volume = {13},
year = {2011}
}
@article{matplotlib,
author = {Hunter, John D.},
doi = {10.1109/MCSE.2007.55},
journal = {Computing in Science {\&} Engineering},
pages = {90},
title = {{Matplotlib: A 2D Graphics Environment}},
url = {http://ieeexplore.ieee.org/document/4160265/},
volume = {9},
year = {2007}
}


@article{Hough:1959,
    author = "Hough, P.V.C.",
    editor = "Kowarski, L.",
    title = "{Machine Analysis of Bubble Chamber Pictures}",
    journal = "Conf. Proc. C",
    volume = "590914",
    pages = "554--558",
    year = "1959"
}

@article{BECEIRONOVO2015,
title = "Active targets for the study of nuclei far from stability",
journal = "Progress in Particle and Nuclear Physics",
volume = "84",
pages = "124 - 165",
year = "2015",
issn = "0146-6410",
doi = "https://doi.org/10.1016/j.ppnp.2015.06.003",
url = "http://www.sciencedirect.com/science/article/pii/S0146641015000459",
author = "S. Beceiro-Novo and T. Ahn and D. Bazin and W. Mittig",
keywords = "Active targets, Gas detectors, Inverse kinematics",
abstract = "Weakly bound nuclear systems can be considered to represent a good testing-ground of our understanding of non-perturbative quantum systems. Reactions leading to bound and unbound states in systems with very unbalanced neutron-to-proton ratios are used to understand the properties of these systems. Radioactive beams with energies from below the Coulomb barrier up to several hundreds MeV/nucleon are now available, and with these beams, a broad variety of studies of nuclei near the drip-line can be performed. To compensate for the low intensity of secondary beams as compared to primary beams, thick targets and high efficiency detection is necessary. In this context, a new generation of detectors was developed, called active target detectors: the detector gas is used as target, and the determination of the reaction vertex in three dimensions allows for good resolution even with thick targets. The reaction products can be measured over essentially 4π. The physics explored with these detectors together with the technology developed will be described."
}