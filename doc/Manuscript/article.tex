\documentclass[preprint,12pt]{elsarticle}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{float}
\usepackage{mathtools}
%\usepackage{minted}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{makecell}
\usepackage{array}
\usepackage{multirow}
\usepackage{mathpazo}
%\usepackage{multicol}
%\usepackage{siunits}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage[toc,page]{appendix}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage[section]{placeins}
\usepackage{pdflscape}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{natbib}
\bibliographystyle{unsrtnat}


\setlength{\marginparwidth}{4cm}
\usepackage{todonotes}
\newcommand{\inner}[2]{\langle #1 | #2 \rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\wij}{W_{ij}}
\newcommand{\loss}{\mathcal{L}}

\journal{Nuclear Instruments and Methods
in Physics Research Section A: Accelerators, Spectrometers,
Detectors and Associated Equipment}

\begin{document}

\begin{frontmatter}

%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Latent Variable Clustering for Classification of Events in the AT-TPC}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}https://www.overleaf.com/project/5e73f990fcbd310001959b01
%% \address[label1]{}
%% \address[label2]{}

\author{R.~Solli}
\address{Department of Physics, University of Oslo, POB 1048 Oslo, N-0316 Oslo, Norway}

\author{D.~Bazin}
\address{Department of Physics and Astronomy and Facility for Rare Ion Beams and National Superconducting Cyclotron Facility, Michigan State University, East Lansing, MI 48824, USA}
\author{M.P.~Kuchera}
\address{Physics Department, Davidson College, Davidson, North Carolina, USA}
\author{M.~Hjorth-Jensen}
\address{Department of Physics and Astronomy and Facility for Rare Ion Beams and National Superconducting Cyclotron Facility, Michigan State University, East Lansing, MI 48824, USA}
\address{Department of Physics and Center for Computing in Science Education, University of Oslo, POB 1048 Oslo, N-0316 Oslo, Norway}
\ead{hjensen@frib.msu.edu}
\ead[url]{http://mhjgit.github.io/info/doc/web/}


\begin{abstract}
In this work we introduce the application of convolutional autoencoder neural networks to the analysis of two-dimensional projections of particle tracks from a resonant proton scattering experiment on ${}^{46}$Ar. 
The data we analyze were recorded by an active target time-projection chamber (AT-TPC). Machine learning presents an interesting avenue for researchers operating an AT-TPC, as traditional analysis methods of AT-TPC data are both computationally expensive and fit all particle tracks against the event type of interest. The latter presents a considerable challenge when the space of reactions is not known prior to the analysis. 

We explore the performance of the autoencoder neural networks and a pre-trained VGG16 \cite{Simonyan2014} convolutional neural network on two tasks: a semi-supervised classification task and the unsupervised clustering of particle tracks. On the semi-supervised task, we find that a logistic regression classifier trained on small labelled subsets of the latent space of these models perform very well. On simulated data these classifiers achieve an $f1$ score \cite{Chinchor1992} of $f1>0.95$. The VGG16 latent classifier achieves this result with as few as $N=100$ samples, as does the convolutional autoencoder when trained on the VGG16 representations of the particle tracks. On real data, pre-processed with noise filtering, the same models achieve an $f1>0.7$. For unfiltered real data the models achieve an $f1$ value larger than $0.6$. Both of the previous results were found with the classifiers trained on $N=100$ samples. Furthermore, we found that the autoencoder model reduces the variability in the identification of proton events by $64\%$ from the benchmark logistic regression classifier trained on the VGG16 latent space on real experimental data. 

On the clustering task, we found that a $K$-means algorithm applied to the simulated data in the VGG16 latent space forms almost perfect clusters, with an adjusted rand index \cite{Hubert1985} ($ARI$) $ > 0.8$.  Additionally, the VGG16+K-means approach finds high purity clusters of proton events for real experimental data. We also explore the application of neural networks to clustering by implementing a mixture of autoencoders algorithm. With this model we improved clustering performance on the real experimental data from an $ARI = 0.17$ to an $ARI = 0.40$. However, the neural network clustering suffers from stability issues necessitating further investigations into this approach. 

\end{abstract}

%%Graphical abstract
\begin{graphicalabstract}
%\includegraphics{grabs}
\end{graphicalabstract}

%%Research highlights
\begin{highlights}
\item Research highlight 1
\item Research highlight 2
\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

\section{Introduction}\label{sec:intro}
\begin{itemize}
    \item Overall goals for the AT-TPC
    \item Nuclear physics motivation for clustering-based (unsupervised) methods
    \item Motivation from ML approach
    
\end{itemize}
The AT-TPC \cite{Bradt2018} is a novel type of detector designed specifically for nuclear physics experiments where the energies of the recoiling particles are very low compared to the energy required to escape target material \cite{}. The luminosity of nuclear physics experiments performed with fixed targets is directly proportional to the amount of material encountered by the beam. On the other hand, for several classes of experiments the detection of recoil particles is paramount, therefore limiting the target thickness. In addition, the properties of the recoil particles are modified while traversing the target material, affecting the resolutions that can be achieved. This necessary balance between luminosity and resolution is particularly difficult when performing experiments with rare isotope beams, because of the low intensities available. 

The concept of active target aims at mitigating this compromise, by turning the target itself into a detector \cite{Beceiro2018}. Most active target detectors such as the AT-TPC are composed of a time projection chamber (TPC) where the detector gas is at the same time the target material. Recoil particles that originate from a nuclear reaction between a beam nucleus and a gas nucleus can be tracked from the vertex of the reaction to their final position inside the active volume of the target. Their properties can therefore be measured without any loss of resolution regardless of the amount of material traversed by the beam. At the same time, the detection efficiency is dramatically increased by the very large solid angle covered in this geometry. A direct consequence of this concept is the inclusiveness of the experimental data recorded by this type of detector: any nuclear reaction happening within the target is recorded. Although this sounds like an advantage from the scientific point of view, it poses great challenges during the analysis phase, that are reminiscent of bubble chamber times and on par with nowadays event classification issues in particle physics. More often than not, the reaction channel of interest has one of the lowest cross sections, therefore the physicist is faced with the task of sorting out the corresponding events from the ``background" of other reaction channels. 

Because TPCs produce 3-dimensional images of charged particle tracks, the event classification task is often akin to a visual inspection (bubble chamber times), which is not practical nowadays because of the large quantities of data. Machine learning techniques then appear as a promising prospect, in particular in the image recognition domain where much progress has been made recently. In addition, some ML algorithms offer new possibilities such as the potential discovery of unforeseen phenomena that would have been missed by more traditional analysis methods. The classification and clustering of events based on various ML algorithms is hereby examined, using experimental data recorded by the AT-TPC during its commissioning experiment from a radioactive $^{46}$Ar beam reacting of an isobutane target composed of proton and carbon nuclei. 

\section{Experimental Details} 
The goal of the experiment was to measure the evolution of the elastic and inelastic cross sections between $^{46}$Ar and protons as a function of energy (also called excitation function), and observe resonances in the composite system $^{47}$K that correspond to analog states in the nucleus $^{47}$Ar. Spectroscopic information can then be obtained from the shape and amplitude of the observed resonances \cite{Bradt2018}. 
The experiment was performed at the National Superconducting Cyclotron Facility (NSCL) where a $^{46}$Ar beam was produced via fragmentation of a $^{48}$Ca beam on a $^9$Be target at about 140 MeV/u. The $^{46}$Ar isotopes were then filtered, thermalized, and finally re-accelerated to 4.6 MeV/u by a linear accelerator. This scheme was used to produce a low emittance beam necessary to guarantee a good energy resolution in the excitation function. Because of the energy loss of the $^{46}$Ar beam particles as they traverse the target gas volume, the position of the reaction vertex along the beam axis is directly related to the energy at which the reaction occurs. This allows the AT-TPC to measure the excitation function over a wide range of energies from a single beam energy.

The detector was placed inside the bore of a MRI solenoid energized to about 2 Tesla. This axial magnetic field served the purpose of bending the trajectories of the recoil particles in order to i) increase their length and ii) provide a measurement of their bending radius, directly related to their magnetic rigidity. Because the recoil particles travel in gas, they slow down and eventually stop, therefore their trajectories are described by 3-dimensional spirals (see \cite{Bradt2017}). One of the difficulties encountered in the analysis is that the shape of these spirals does not have any analytical form because it follows the energy-loss profile of the particles. It therefore needs to be simulated via an integration, which is numerically costly. Other difficulties are related to several experimental effects that deteriorate the quality of the data, namely saturation and cross-talk effects, as well as random noise. 

The method used in \cite{Bradt2018} to analyze the data followed a 3-phase sequence: cleaning, filtering and fitting. Traditional methods were used to perform each of these tasks, and ultimately extract the scientific information, but there were severe limitations and high computational costs that would have become prohibitive had the data set been larger by an order of magnitude.
\todo{Should I give some details about how the 3 tasks were performed?}

\section{Methods}\label{sec:methods}

\subsection{Classifying  events} 
\todo{Brute force serving as motivation for why we do unsupervised learning.}
The filtering task relied on a set of $\chi^2$ distributions based on different criteria, however there was no easy way to quantify the effectiveness of the event selection from the thresholds applied to these distributions. In addition, the filtering task produced a binary result only, "good" or "bad", while the data clearly contained more classes of events, if only because of the presence of carbon in the target (from the isobutane gas). In a broader perspective, an unsupervised classification algorithm would offer the possibility to "discover" rare events not anticipated (or overlooked) by the physicist. From a practical point of view, compared to supervised learning, it also avoids the necessary labeling task of the learning set events, which is error prone and time consuming.

\subsection{Why machine learning}

\begin{itemize}
    \item Traditional MC methods fall short in two principal ways:
    \begin{itemize}
        \item The computational cost per event is too large given the size of the data-sets
        \item The broken tracks, noisy environment creates bad fit statistics for otherwise useful events. End result is $f1 \sim 0.7 $
    \end{itemize}
\end{itemize}

The $\chi^2$ approach used in the traditional analysis performed on the $^{46}$Ar data is very expensive computationally, because it involves the simulation of thousands of tracks for each recorded event, that need to be performed during the Monte-Carlo fitting sequence. Even though the reaction of interest in the $^{46}$Ar experiment had the largest cross section (elastic scattering), the sheer number of events to analyse (in the hundreds of thousands) necessitated the use of parallel processing on a high performance computer cluster. In the case of an experiment where the reaction of interest would represent less than a few percents of the total cross section, this procedure would become highly inefficient and prohibitive. Most of the CPU time would be spent on simulating events that are not of interest. A ML algorithm able to classify the data without a priori knowledge of the different types of events would increase the efficiency of the analysis tremendously, by concentrating the fitting efforts on events of interest only.

\subsection{Data}
 In this section we give a brief overview of the data, for a more in-depth consideration we refer to \cite{Mittig2015}, \cite{Suzuki2012} and  \cite{Bradt2017a}. 

In this thesis we will explore the machine learning algorithms described in chapters \ref{ch:ml} and \ref{ch:autoencoder} on three datasets from the AT-TPC, listed in table \ref{tab:datasets}. The simulated data will serve as a baseline for performance, while the filtered and full data are real records from the ${}^{46}$Ar experiment. Each of these datasets have different distributions of the constituent classes. We list these differences in table \ref{tab:class_distr}. The filtered data differs from the full dataset as it has some post-processing applied to remove noise. This is not the case for the full data. The details of the post-processing are given in later sections. First, we consider the pipeline from raw data to images that the algorithms described in chapter \ref{ch:autoencoder} can process. 

\begin{table}
\centering
\caption{Descriptions of number of events in the data used for analysis in this thesis. In principle we can simulate infinite data, but it is both quite simple and not very interesting outside a case for a proof-of-concept}\label{tab:datasets}
\begin{tabular}{lccc}
\toprule
{} & Simulated & Full & Filtered \\
\midrule
Total &  $8000$ & $51891$ & $49169$ \\
labelled & $2400$ & $1774$ &  $1582$ \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{Event class percentages for each of the datasets used in the analysis conducted in this thesis. The decrease in the "other" class of events from full to filtered data owes to the thresholding of events. If an event contains fewer than $20$ data-points it is discarded}\label{tab:class_distr}
\begin{tabular}{lccc}
\toprule
{} & Simulated & Full & Filtered \\
\midrule
$\%$ Proton & $50$ & $25.3$ & $27.6$ \\ 
$\%$ Carbon & $50$ & $12.0$ & $11.9$ \\
$\%$ Other & $0$ & $62.8$ & $60.1$ \\
\end{tabular}
\end{table}

\subsection{Data processing}

Our data processing pipeline begins with localized point-cloud data in the two-dimensional detector coordinate system, with one time dimension and a corresponding charge measurement. The charge and time measurement are extracted as the peak of this signal over the event, resulting in a maximum of one measurement per pad in the sensor plane. The events range from fairly sparse $< 10^2$ records to being very populated, depending on where in the volume the reaction occurs as well as other noise-generating factors. We centre the charge data to values $>1$ by adding the lowest occurring record in the run and apply a log-transform to get values in $\R^+$. Subsequently, we scale by the maximum value in the run to get charge data in the interval $[0, 1]$. Lastly, the transformed charge data is saved in a two-dimensional $128px\, \times \,128px$ array using the \lstinline{matplotlib} package provided in the \lstinline{Python} programing language \cite{matplotlib}. The choice of scaling was made to accommodate a binary cross-entropy loss on a 2D projection, as it presumes the true values to be bounded as probabilities.

We will begin by considering the simulated events, followed by subsequent considerations of the full and filtered experimental data.

\subsection{Simulated \texorpdfstring{${}^{46}$Ar}{46Ar}  events}\label{sec:data_sim}

The simulated AT-TPC tracks were simulated with the \lstinline{pytpc} package developed by \citet{Bradt2017a}. Using the same parameters as for the ${}^{46}$Ar$(p, p)$ experiment, a small set of $N=4000$ events were generated per class, as well as a larger set of $N=40000$ events per class. The events are generated as point-clouds, consisting of position data on the x-y plane, a time-stamp and an associated charge value. These point-clouds are transformed into pure x-y projections with charge intensity for the analysis in this thesis using the pipeline described in the previous section. 

More formally the events are originally composed of peak-only 4-tuples of $e_i = (x_i, y_i, t_i, c_i)$. The peak-only designation indicates that we use the recorded peak amplitude on each pad, the tuples then correspond to pads that recorded a signal for that event. Each event is then a set of these four-tuples: $\epsilon_j = \{e_i\}$ creating a track in three-dimensional space. 

To emulate the real-data case, we set a subset of the simulated data to be labelled and treat the rest as unlabelled data. We chose this partition to be $15\%$ of each class. We denote this subset and its associated labels as $\gamma_L=(\boldsymbol{X}_L, \boldsymbol{y}_L)$, the entire dataset which we will denote as $\boldsymbol{X}_F$. To clarify, please note that $\boldsymbol{X}_L \subset \boldsymbol{X}_F$.

We display two simulated events in figure \ref{fig:sim_samples}. The top row illustrates a proton-event, and the bottom a carbon-event. 

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{plots/display_eventssimulated.pdf}
\caption[Displaying simulated events in 2D and 3D]{Two- and three-dimensional representations of two events from a simulated ${}^{46}$Ar experiment. Each row is one event in two projections, where the color intensity of each point indicates higher charge values recorded by the detector.}\label{fig:sim_samples}
\end{figure}


\subsection{Full \texorpdfstring{${}^{46}$Ar}{46Ar}  events}\label{sec:data_real}

The events analyzed in this section were retrieved from the ${}^{46}$Ar resonant proton scattering experiment recorded with the AT-TPC. 

The sensor plane in the AT-TPC is very sensitive, as such there is substantial noise in the ${}^{46}$Ar data. The noise can be attributed to structural noise from electronics cross-talk, and possible interactions with cosmic background radiation, as well as other sources of charged particles. Part of the challenge for this data then comes from understanding of the physics of the major contributing factors to this noise. 

We display two different events from the ${}^{46}$Ar experiment in figure \ref{fig:samples}. The top row illustrates an event with a large fraction of noise, while the bottom row shows an event nearly devoid of noise. The very clear spiral structure of the bottom row indicates that this is a proton-event.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{plots/display_eventsfull_.pdf}
\caption[Displaying un-filtered events in 2D and 3D]{Two- and three-dimensional representations of two events from the ${}^{46}$Ar experiment. Each row is one event in two projections, where the color intensity of each point indicates higher charge values recorded by the detector.}\label{fig:samples}
\end{figure}

\subsection{Filtered \texorpdfstring{${}^{46}$Ar}{46Ar} events}\label{sec:filtered}

As we saw in the previous section, the detector picks up a significant amount of noise. We split the noise broadly in two categories,  one being random-uncorrelated noise and the second is structured noise. The former can be quite trivially removed with a nearest-neighbour algorithm that checks if a point in the event is close to any other. To remove the correlated noise, researchers at the NSCL developed an algorithm based on the Hough transform \cite{Newman1972}. This transformation is a common technique in computer vision, used to identify common geometric shapes like lines and circles. Essentially, the algorithm draws many lines (of whatever desired geometry) through each data-point and checks whether these lines intersect with points in the dataset. Locations in parameter space that generate many intersections then become bright spots, allowing us to filter away points that are not close to these points. These algorithms remove a large amount of the unstructured noise and are computationally rather cheap.

We illustrate two filtered events in figure \ref{fig:samples_filtered}. These are the same events as shown in figure \ref{fig:samples}, but with the Houghes' and nearest neighbours filtering applied. 

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth, height=9cm]{plots/display_eventsclean_.pdf}
\caption[Displaying filtered events in 2D and 3D]{Two- and three-dimensional representations of two events from the ${}^{46}$Ar experiment. Each row is one event in two projections, where the lightness of each point indicates higher charge values. These events have been filtered with a nearest neighbors algorithm and an algorithm based on the Hough transform, described in section \ref{sec:filtered}}\label{fig:samples_filtered}
\end{figure}


\section{Machine Learning Methods}

\begin{itemize}
    \item Reiterate the aim of unsupervised  clustering of events.
    \item Challenges from a machine learning perspective. 
    \begin{itemize}
        \item Supervised vs. Unsupervised learning
        \item Define Classifcation and Clustering 
        \item Define latent space (refer)
        \item Traditional unupervised - distances in high dimensional spaces etc. 
    \end{itemize}
    \item Digression here to explain some ML concepts? Link back to Conv part of Michelles paper? 
    \item Opportunities presented by: 
    \begin{itemize}
        \item Transfer learning (train on VGG use on AT-TPC)
        \item Compression is understanding (leverage autoencoders) 
    \end{itemize}
    \item Building on Kuchera et. al we investigate the VGG16 networks application. 
    \item Novel contribution by applying clustering autoencoder networks. 
\end{itemize}

\subsubsection{Pre-trained neural networks}

To affect the clustering of AT-TPC events, we first considered the results from \citet{Kuchera2019}. In their work, the authors build a very successful classification approach using a pre-trained neural network to preprocess the events. From their results, it is clear that the pre-trained network captures salient information about the difference in event types. Our approach builds on previous work by considering the output from the pre-trained model as a representation on which we perform clustering with a simple K-means algorithm. 

In the Machine Learning community, it is not uncommon to publish packaged models with fitted parameters from image recognition contests. These models are trained on datasets with millions of images and classify between hundreds of distinct classes; one such is the imagenet dataset. In their work, \citet{Kuchera2019} used the VGG16 architecture trained on imagenet to classify AT-TPC. Following on their work we will be applying the same VGG16 model to the clustering task. 

The VGG16 network is one of six analogous networks proposed by \citet{Simonyan2014}. They were runners up in the ISLVRC(ImageNet large scale visual recognition competition) of 2014 \cite{Russakovsky2015}, narrowly being beat by GooLeNet. The network architectures are fairly straightforward; for VGG16, there are sixteen layers in the network. The first thirteen of which are convolutional layers with exclusively $3 \times 3$ kernels. The choice of the kernel size is based on the fact that a stacked $3 \times 3$ kernel is equivalent to larger kernels in terms of the receptive field of the output. Three $3 \times 3$ kernels with stride $1$ have a $7 \times 7$ receptive field, but the larger kernel has $81\%$ more parameters and only one non-linearity \cite{Simonyan2014}. Stacking the smaller kernels then contributes to a lower computational cost. Additionally, there is a regularizing effect from the lowered number of parameters and increased explanatory power from the additional non-linearities. The full architecture is detailed in appendix \ref{tab:vgg}.

\subsubsection{Deep clustering: Mixture of autoencoders}\label{sec:mixae}

As an alternative to using a pre-trained model in conjunction with a clustering algorithm, we have also considered bespoke end-to-end clustering models. These models are optimized specifically to the AT-TPC data with the objective of clustering. For this paper, we considered the deep convolutional embedded clustering (DCEC) by \citet{Guo2017} as well as the mixture of autoencoders (MIXAE) method introduced by \citet{Zhang}. While we were able to reproduce the authors' results on their data, the DCEC algorithm proved unable to cluster AT-TPC in our implementation. However, this provides valuable insight as the seeds of the clusters are constructed by a K-means algorithm. This insight contrasts with our positive results from applying a pre-trained model with k-means and highlights potentially significant differences in models trained on a supervised or unsupervised objective for clustering tasks in nuclear physics. 

The MIXAE algorithm clusters data by pairing a set of encoder-decoder neural networks (autoencoder) with an auxiliary assignment network: each autoencoder constructs a representation of a given event while the assignment network assigns a set of cluster probabilities to the event. As noted by \citet{Zhang}, the MIXAe model assumes that the clusters are roughly similar in volume in the data and that one has some reasonable estimate on the number of clusters present. The architecture is portrayed in figure \ref{fig:mixae}, tapered boxes denote a direction of compression in the network components. 

As the MIXAE model does not have access to ground truth labels the training consists of applying simple heuristics that constitute a necessary, but not sufficient, fulfilment for a high-quality MIXAE model given the assumptions made of the model. The heuristics can be stated as: 
\begin{enumerate}
	\item  Given a clustering assignment to a particular autoencoders' representation that autoencoders' reconstruction should be similar to the input event. 
	\item Within a batch of events presented to the model, there should be a spread in the clustering assignment between all clusters. 
	\item Each clustering prediction should be as close to a strong prediction as possible, i.e. assigning high probabilities is preferable to weak assignments.
\end{enumerate}

\noindent For a more formal consideration of these heuristics see \citet{Zhang}.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\textwidth]{plots/mixae.pdf}
	\caption[Mixture of autoencoders schematic]{Schematic of a MIXAE model. A sample $\hat{\boldsymbol{x}}$ is compressed to set of lower-dimensional representations $\{\boldsymbol{z}^{(i)}\}$ by $N$ autoencoders. These samples are concatenated and passed through an auxiliary assignment network that predicts a confidence of cluster belonging for each autoencoder. For further details see the text. Figure adapted from \citet{Zhang}}
	\label{fig:mixae}
\end{figure}

\section{Results and Discussions}\label{sec:results}
The principal challenge in the AT-TPC experiments that we are trying to solve is the reliance on labelled samples in the analysis as future experiments may not have as visually dissimilar reaction products  as we observe in the ${}^{46}$Ar experiment.  The  ${}^{46}$Ar experiment does, however, provide a useful example where we can then explore unsupervised techniques. In this chapter, we explore the application of clustering techniques to events represented in latent spaces. 

We begin by exploring a naive K-means approach on the latent space of a pre-trained network. Subsequently, we investigate other clustering methods and two autoencoder based clustering algorithms, as outlined in section \ref{sec:deep_clustering}.

\todo{Drop this argument or build it in the ML section?}
This chapter builds on the previous results from semi-supervised classification. We observe that we can construct high-quality latent spaces. These high-quality spaces facilitate an investigation of clustering techniques. 

\todo{Reword this to not contrast against semi-supervised}
The approach for clustering of events is different from the semi-supervised approach in two meaningful ways. First, it is a harder task, as we will demonstrate. The clustering task thus necessitates a more exploratory approach to the problem. Second, as a consequence of the challenge, the focus will be a bit different than for the semi-supervised approach. We will still utilize the same architectures and models starting with a search over the parameter space over which we measure the performance using the adjusted rand score (ARS) and accuracy defined in section \ref{sec:unsupervised_perf} and \ref{sec:supervised_perf}, respectively.

As with chapter  \ref{chap:classification} where we explored the semi-supervised results, we begin this chapter by considering the VGG16 pre-trained model as a benchmark.

Lastly, we note that the focus of this work is mainly on discovering possible avenues for further research. This focus requires a broad scan of possible avenues rather than a rigorous analysis of one specific model.
\subsection{Transfer learning}
As in chapter \ref{ch:architectures}, we also use the VGG16 pre-trained network as a baseline for the clustering performance. We begin by considering a classical K-means approach to clustering. However, the output from the VGG16 network is very high dimensional with output vectors in $\R^{8192}$. \todo{reword} One of the primary concerns is then the curse of dimensionality, where the ratio of distances goes to one with increasing dimensionality as shown by \citet{Aggarwal}. However, one of the central caveats to the authors  finding is that the elements are uniformly distributed in the latent space. It is then possible that all the class information lies in some sub-space of the latent data. To investigate this, we perform clustering analysis using the full representation and the $10^2$ first principal components only. 

\subsection{K-means}

We begin by investigating the K-means clustering algorithm on the VGG16 latent space. As in chapter \ref{chap:classification} the VGG16 model is pre-trained on the imagenet dataset creating a set of vectors $\boldsymbol{x} \in \R^{8192}$. To cluster we use \lstinline{scikit-learn} implementation of the K-means algorithm, with default parameters \cite{Pedregosa2011}. The results of the clustering runs are included in table \ref{tab:clstr_vgg}. We observe that we are able to attain near-perfect clustering on simulated data and that there is a sharp decline in performance as we add noise by moving to the filtered and full datasets. 

\begin{table}[H]
\centering 
\caption[K-means on pre-trained model]{K-means clustering results on AT-TPC event data. We observe that the performance predictably decreases with the amount of noise in the data.}\label{tab:clstr_vgg}
\input{./plots/kmeans_vgg.tex}
\end{table}

In addition to the performance measures reported in table \ref{tab:clstr_vgg}, it is interesting to observe which samples are being wrongly assigned. We achieve this by tabulating the assignments of samples relative to their ground truth labels. From these tables, we can infer which classes are more or less entangled with others. We tabulate the results for each dataset is in figure \ref{fig:clster_confmat}. We observe that the proton class is consistently assigned in a pure cluster. Purity is inferred by how much spread there is in the column between the ground truth labels. A high-quality cluster will, in addition to being pure, also capture most entries the class represented by the cluster. For example, consider the row corresponding to the proton class in figure \ref{fig:clster_confmat}. The column corresponding to the largest entry in the proton row has zero other predicted classes in it. From this, we conclude that the proton cluster is a high quality, high purity cluster. 

\begin{figure}
\centering

	\subfloat{
	\includegraphics[width=0.35\textwidth]{./plots/Simulatedvgg_pca_conf_mat.pdf}

}
	\hspace{-1cm}
	\subfloat{
	\includegraphics[width=0.35\textwidth]{plots/Filteredvgg_pca_conf_mat.pdf}
}
	\hspace{-1cm}
	\subfloat{
	\includegraphics[width=0.35\textwidth]{plots/Fullvgg_pca_conf_mat.pdf}
}
\caption[Pre-trained network - confusion matrices]{Confusion matrices for the K-means clustering of simulated, filtered and full AT-TPC events. The true labels indicate samples belonging to the p (proton), carbon (C), or other classes. }\label{fig:clster_confmat}
\end{figure}

We repeat this analysis using a PCA dimensionality reduction on the latent space of the VGG16 model. This is done to estimate to what degree the class separating information is encoded in the entirety of the latent space, or in some select regions. The results from the PCA analysis were virtually identical to the results sans the PCA, and so we omit them for brevity. 

Furthermore, we wish to characterize further the clusters presented in figure \ref{fig:clster_confmat}. To achieve this, we sample from the proton samples belonging to different clusters for the filtered and full data.

\begin{figure}
\includegraphics[width=\textwidth]{plots/filtered_vgg_cluster_repr.pdf}
\caption[Filtered proton samples by cluster belonging]{Illustrating a sample of proton events from different K-means clusters from the filtered dataset. Each row belongs to a single cluster corresponding to the filtered confusion matrix in figure \ref{fig:clster_confmat}}\label{fig:filtered_vgg_clster_repr}
\end{figure} 

\begin{figure}
\includegraphics[width=\textwidth]{plots/full_vgg_cluster_repr.pdf}
\caption[Full proton samples by cluster belonging]{Illustrating a sample of proton events from different K-means clusters from the full dataset. Each row belongs to a single cluster corresponding to the full confusion matrix in figure \ref{fig:clster_confmat}}\label{fig:full_vgg_clster_repr}
\end{figure} 

In addition to the results presented in this section, we performed clustering with a number of different algorithms included in the \lstinline{scikit-learn} package. None of them provided any notable differences from the K-means results or were significantly worse. Notably, the DBSCAN algorithm failed to provide any useful clustering results. We find this important as one of the significant drawbacks of K-means, and the deep clustering algorithms presented in section \ref{sec:deep_clustering}, is that they are all dependent on pre-determining the number of clusters. This is not the case for DBSCAN. 
\subsection{Autoencoder clustering}

In the previous section we demonstrated a potential for clustering of events from an AT-TPC experiment. To build on this result we will in this section explore the application of the mixture of autoencoders (MIXAE) algorithm introduced in section \ref{sec:deep_clustering}. The other algorithm introduced in section \ref{sec:deep_clustering}, deep clustering with convolutional autoencoders (DCEC), consistently collapsed to just one cluster for all datasets.

In the MIXAE algorithm the hyper-parameters to adjust are all the ordinary parameters that we introduced in table \ref{tab:convae_hyperparams}. In addition to those parameters we have the weighting of the loss terms: $\theta$, $\alpha$ and $\gamma$. These weighting parameters are attached to the reconstruction loss, sample entropy and batch-wise entropy respectively. \cite{Zhang} note that these parameters are very important for the model performance and so we focus primarily on these. 

\todo{but why}We empirically freeze the convolutional autoencoder hyperparameters to compress the original $128 \times 128$ to a $8 \times 8$ pixel grid using four convolutional layers. The parameters chosen for the autoencoders are listed in full in table \ref{tab:mixe_ae_hyperparams}.

\begin{table}[H]
\todo{prettify or remove}
\renewcommand*{\arraystretch}{0.5}
\centering
\caption{Hyperparameters selected for the autoencoder components of the MIXAE algorithm, see table \ref{tab:convae_hyperparams} for a full description of the parameters.}\label{tab:mixe_ae_hyperparams}
\setlength{\extrarowheight}{15pt}
\hspace*{-0.5in}
\begin{tabular}{ll}
\toprule
Hyperparameter & Value \\
\midrule
\multicolumn{2}{l}{Convolutional parameters: } \\
\midrule
Number of layers & $4$ \\
Kernels & $[3,\,3,\,3,\,3]$\\
Strides & $[2,\,2,\,2,\,2]$ \\
Filters & $[64,\, 32, \,16, \,8,]$ \\ 
\midrule
\multicolumn{2}{l}{Network parameters: } \\
\midrule
Activation & LReLu \\
Latent type & None \\
Latent dimension & 20  \\
$\beta$ & N/A \\
Batchnorm & False \\
\midrule
\multicolumn{2}{l}{Optimizer parameters: } \\
\midrule
$\eta$ & $10^{-3}$ \\
$\beta_1$ & $0.9$ \\
$\beta_2$ & $0.99$ \\
\bottomrule
\end{tabular}
\end{table}

Since there are then only three remaining hyperparameters we choose to perform a coarse grid-search as described in section \ref{sec:hyperparam_search_arch}.

\begin{table}
\centering
\caption{Hyperparameter grid for the MIXAE loss weighting terms. The grid is given as exponents for logarithmic scales.}\label{tab:mixae_loss_weights}
\begin{tabular}{lll}
\toprule
Parameter & Grid & Scale \\
\midrule 
$\theta$ & $[-1,\, 5]$ & Logarithmic \\
$\alpha$ & $[-5,\, -1]$ & Logarithmic \\
$\gamma$ & $[3,\, 5]$ & Logarithmic
\end{tabular}
\end{table}

\subsection{Simulated AT-TPC data}

To train the MIXAE clustering algorithm, we use the large simulated dataset with $M=80000$ points, evenly distributed between proton- and carbon-events. The algorithm is trained on a subset of $60000$ of these samples, and we track performance on the remaining $20000$ events. 

The grids selected for the search are listed in table \ref{tab:mixae_loss_weights}. The search yielded an optimal configuration with 

\begin{align}
\theta = 10^{-1}, \\
\alpha = 10^{-2}, \\
\gamma = 10^5.
\end{align}

Finally, for these parameters we re-ran the algorithm $N=10$ times to investigate the stability of the algorithm. The results are reported in figure \ref{fig:mixae_sim}. We observe that while the algorithm can achieve very strong performance, with an adjusted rand index (ARI) $ > 0.8$, it fluctuates strongly with repeated experiments. The $ARI$ is a clustering performance measure similar to accuracy, but adjusted for chance. We elaborate on the $ARI$ score in section \ref{sec:unsupervised_perf}. As mentioned in section \ref{sec:mixae} the batch-entropy has a second minimum when the cluster confidences are near equal. It is this behavior we observe in figure \ref{fig:mixae_sim}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{./plots/sim_mixae.pdf}
\caption[Clustering performance of MIXAE on simulated AT-TPC data]{Performance for the MIXAE model on simulated AT-TPC data. In the top row the loss components are plotted for each run, and in the bottom row the adjusted rand index (ARI) and clustering accuracy are shown. Each run is color-coded with the ARI achieved at the end of the run. }\label{fig:mixae_sim}
\end{figure}

\subsection{Filtered AT-TPC data}

We repeat the optimization steps in the previous section for the filtered AT-TPC data. The exception being that we allow the algorithm to train on the labelled samples. Beginning with a wide grid equal to the grid used for the simulated data we searched over all parameter configurations to find promising values. We then performed a local search around these values to pin-point the hyperparameter configuration. This search yielded the optimal hyper-parameters 

\begin{align}
\theta &= 10^{1}, \\
\alpha &= 10^{-1}, \\
\gamma &= 3.162\times 10^3.
\end{align}

\noindent The results of the runs are included in figure \ref{fig:mixae_clean}. We observe ghat the highest performing models reach an $ARI > 0.5$, which is higher than the performance achieved by the K-means algorithm applied to the VGG16 latent space. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{./plots/clean_mixae.pdf}
\caption[Clustering performance of MIXAE on filtered event AT-TPC data]{Performance for the MIXAE model on filtered AT-TPC data. In the top row the loss components are plotted for each run, and in the bottom row the adjusted rand index (ARI) and clustering accuracy are shown. Each run is color-coded with the ARI achieved at the end of the run.}\label{fig:mixae_clean}
\end{figure}


\subsection{Full AT-TPC data}

As in the two previous sections, we repeat the same procedure of iterative grid searches on the MIXAE loss-weights. Each configuration is re-run a total of $N=10$ times to capture fluctuations in the performance before a final selection is made on the hyperparameters. For the full dataset the MIXAE hyperparameters converge to the same values as for the clean data, i.e. 


\begin{align}
\theta &= 10^{1}, \\
\alpha &= 10^{-1}, \\
\gamma &= 3.162\times 10^3.
\end{align}

\noindent As with the previous datasets we include a plot of the loss curves and performance measures for $N=10$ runs with the same loss-weight parameters, shown in figure \ref{fig:mixae_full}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{./plots/real_mixae.pdf}
\caption[Clustering performance of MIXAE on full event AT-TPC data]{Performance for the MIXAE model on un-filtered AT-TPC data. In the top row the loss components are plotted for each run, and in the bottom row the adjusted rand index (ARI) and clustering accuracy are shown. Each run is color-coded with the ARI achieved at the end of the run.}\label{fig:mixae_full}
\end{figure}

\subsection{Comparing performance}

It is also interesting to compare and contrast the clustering results from the MIXAE model with those of the VGG16$+$K-means outside the fairly abstract accuracies and rand scores. It is especially interesting to compare the cluster assignments, as they can inform further research efforts in the clustering of AT-TPC events. We illustrate the clustering with confusion matrices that are shown in figure \ref{fig:mixae_confmat}. From these matrices, we observe that the MIXAE applied to the clean data correctly clusters the noise events. Additionally, it identifies two proton clusters. We observe that these proton clusters are both less pure than the VGG16+K-means clusters and that there does not seem to be a visually meaningful difference between these clusters. The latter is inferred from figure \ref{fig:filtered_mixae_clster_repr}. 

Applied to the real data the MIXAE correctly separates the proton class, however it is unable to separate the carbon events from the amorphous noise events or from the proton cluster. 

\begin{figure}[H]
\centering

	\subfloat{
	\includegraphics[width=0.5\textwidth]{plots/filtered_mixae_conf_mat.pdf}
}
	\hspace{-1cm}
	\subfloat{
	\includegraphics[width=0.5\textwidth]{plots/full_mixae_conf_mat.pdf}
}
\caption[MIXAE - confusion matrices]{Confusion matrices for the MIXAE clustering algorithm on filtered and full AT-TPC events. The true labels indicate samples belonging to the p (proton), carbon (C), or other classes. }\label{fig:mixae_confmat}
\end{figure}

Lastly we wish to further investigate if there are systematic differences between proton events that were placed in different clusters for the clean and full data. From figure \ref{fig:mixae_confmat} we see that the MIXAE algorithm creates two proton clusters for the filtered data, and places about fifty per-cent of the proton events in a cluster with the amorphous "other" events. We extract some proton events from these clusters to inspect whether systematic differences occur. 


\begin{figure}
\centering
\includegraphics[width=\textwidth]{plots/clean_cluster_repr.pdf}
\caption[Selection of carbon events in differing clusters]{Selection of carbon events belonging to different clusters. Each row represents one of the clusters for the filtered data as shown in figure \ref{fig:mixae_confmat}. There seems to be no clear distinction between these rows.}\label{fig:filtered_mixae_clster_repr}	
\end{figure} 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{plots/real_cluster_repr.pdf}	
\caption[Selection of proton events in differing clusters]{Selection of proton events belonging to different clusters. Each row represents one of the clusters for the full data as shown in figure \ref{fig:mixae_confmat}. We observe a tendency for the more noisy events to be placed in the "other" cluster}\label{fig:full_mixae_clster_repr}	
\end{figure} 

\section{Conclusions and Perspectives}\label{sec{conclusion}}

\appendix  
\section{Neural network architectures}

\begin{table}
\caption[vgg architectures]{Showing the details of the VGG network architectures. Network D trained on the ImageNet \cite{Russakovsky2015} dataset the network known as  VGG16 and is what we use in this thesis.}\label{tab:vgg}
\includegraphics[width=\textwidth]{plots/vgg_architectures.png}
\end{table}

\bibliography{bibliography}
\end{document}
